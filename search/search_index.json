{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"annotation_tool/","title":"The Neurobagel Annotation Tool","text":"<p>Neurobagel's annotation tool takes BIDS-style phenotypic data and corresponding data description files and gives users the ability to annotate their data using the Neurobagel data model for preparation to inject that modeled data into Neurobagel's graph database for federated querying.</p> <p>The annotation tool is a Vue application, developed in JavaScript using a variety of tools including Nuxt, Cypress, and BootstrapVue.</p>"},{"location":"annotation_tool/#quickstart","title":"Quickstart","text":"<p>The demo version of the annotation tool is hosted at https://annotate.neurobagel.org/.</p>"},{"location":"annotation_tool/#local-installation","title":"Local Installation","text":""},{"location":"annotation_tool/#building-and-running","title":"Building and running","text":"<pre><code># First, clone the repository at https://github.com/neurobagel/annotation_tool\n\n# Install dependencies\n$ npm install\n\n# Serve with hot reload at localhost:3000\n$ npm run dev\n\n# Build for production and launch server\n$ npm run build\n$ npm run start\n\n# Generate static project\n$ npm run generate\n</code></pre>"},{"location":"annotation_tool/#deployment","title":"Deployment","text":"<p>To deploy the static build on GH pages, run</p> <pre><code>npm run generate\nnpm run deploy\n</code></pre> <p>See the Nuxt documentation for more details.</p>"},{"location":"annotation_tool/#developer-information","title":"Developer information","text":"<p>use node v16.x LTS!</p> <p>  The Annotation Tool is built with the Nuxt framework and currently depends on Nuxt2.  Nuxt2 does not support node versions beyond the v16 LTS (see e.g. this Github issue). If you want to run the tool locally, make sure you are using node v16.x. A good way to manage different node versions is to use the node version manager tool.</p>"},{"location":"annotation_tool/#annotation-workflow","title":"Annotation Workflow","text":"<ol> <li>Upload data table (and/or data dictionary)</li> <li>Linking columns you want annotated with Neurobagel metadata categories</li> <li>Annotate the values of the those categirzed columns</li> <li>Download an annotated version of a BIDS-style data dictionary for your dataset</li> </ol>"},{"location":"annotation_tool/#general-navigation","title":"General Navigation","text":"<p>There are two means of moving forward to the next step in the annotation tool's workflow: (1) the navbar at the top right of the screen which features the page names, and (2) the next page buttons on the bottom right of each page.</p> <p></p> <p>However, special criteria for each page need to be be met in order to move forward. Instructions are offered above the next page button while those conditions are still as of yet unmet. (These are denoted below under Next page criteria.) After having done so, the next page's name in the navbar will turn from gray to green, and the next page button will turn from gray to green as well.</p>"},{"location":"annotation_tool/#page-instructions","title":"Page Instructions","text":""},{"location":"annotation_tool/#home-upload-page","title":"Home (Upload) page","text":"<p>The Home page is where you can upload data tables and dictionaries either for a brand new annotation or to continue a previous annotation session.</p> <p></p>"},{"location":"annotation_tool/#selecting-a-data-table-participantstsv","title":"Selecting a data table (participants.tsv)","text":"<ol> <li>Click the 'Choose File' button in the 'Data Table' section of the page to upload the BIDS-style phenotypic data file you wish to annotate. (One typical name for this file is <code>participants.tsv</code>.)</li> <li>Once uploaded a preview of the tsv file will be shown in the 'Data Table' text box. You may want to examine the preview contents in the text box to ensure you have uploaded the file that you expect to be annotating.</li> </ol> <p>While the annotation tool does not transform data with annotations, it will link the columns found in the uploaded tsv file to categories Neurobagel's metadata schema. Which columns the tool links is up to you on the next page: the categorization page.</p>"},{"location":"annotation_tool/#selecting-a-data-dictionary-optional-participantsjson","title":"Selecting a data dictionary (optional) (participants.json)","text":"<ol> <li>In order to upload your data dictionary, a data table must be uploaded first. This allows the annotation tool to understand the full structure of your data and the add its own entries for annotation purposes to a new, enhanced version of the data dictionary you will be uploading.</li> <li>Click the now-enabled 'Choose File' button in the 'Data Dictionary' section of the page to uploads the BIDS-style data dictionary.</li> <li>Once uploaded a preview of the json file will be shown in the 'Data Dictionary' text box. You may want to examine the preview contents in the text box to ensure you have uploaded the file that you expect to be annotating.</li> </ol> <p>Next page criteria: A <code>participants.tsv</code> file must be uploaded to proceed to categorization of that dataset's columns</p>"},{"location":"annotation_tool/#categorization-page","title":"Categorization page","text":"<p>The Categorization page is where you link the columns in your data table to the categories found in Neurobagel's metadata schema. Current categories include 'Subject ID', 'Age', 'Sex' and 'Diagnosis'. Aside from the 'Subject ID' which is a special case, categories represent different data types, 'Sex' and 'Diagnosis' are categorical while 'Age' is continuous.</p> <p></p>"},{"location":"annotation_tool/#categorizing-data-table-columns","title":"Categorizing data table columns","text":"<ol> <li>Select a category in the category selection component on the left.</li> <li>Select a column from your uploaded <code>participants.tsv</code> in the table on the right. (Its description from your <code>participants.json</code> - if uploaded - will be seen here as well.) This will paint the column's table row with the same color as the category you have selected. This column is now linked to that category and you will be able to annotate its values on the annotation page.</li> </ol> <p>Next page criteria: A column must be annotated as holding a 'Subject ID', and at least one other column must be linked with one of the other categories</p>"},{"location":"annotation_tool/#annotation-page","title":"Annotation page","text":"<p>The Annotation page is where you can annotate the values in your uploaded <code>participants.tsv</code>. The Neurobagel categories which you have linked columns to on the previous Categorization page are listed as tabs on the left.</p> <ul> <li>Clicking on each category tab will allow you to use the interface to annotated the values of each categorized column.</li> <li>When multiple columns have been linked with a category, each column will have its own column in the annotation components on the page.</li> <li>Any value in the annotation interface can be marked as a 'missing value' (i.e. <code>'N/A'</code>, empty string, etc.)</li> </ul> <p></p>"},{"location":"annotation_tool/#age-rows-continuous-values","title":"Age rows (continuous values)","text":"<p>All values for columns categorized as <code>Age</code> can be annotated here with a set of continuous value transformations. Clicking on the dropdown will allow you to select a transformation. Current transformations available include <code>bounded</code>, <code>euro</code>, <code>float</code>, <code>int</code>, and <code>iso8601</code>. The raw values from the column are shown and when a transformation type is selected a preview of how that transformation would alter each raw value is shown.</p>"},{"location":"annotation_tool/#sex-and-diagnosis-rows-categorical-values","title":"Sex and Diagnosis rows (categorical values)","text":"<p>All values for columns categorized as either <code>Sex</code> or <code>Diagnosis</code> can be annotated here with labels that come from Neurobagel and the controlled vocabularies it utilizes to help form the Neurobagel metadata model. Clicking on the drop down on each raw value row will allow you to select an appropriate Neurobagel label for this value.</p> <p></p>"},{"location":"annotation_tool/#missing-values","title":"Missing values","text":"<p>Any continuous or categorical value can be marked as 'missing' via the adjacent <code>Mark as missing</code> button. This will remove the value from the annotatable values and place it in the <code>Missing values</code> section on the page. This can be undone for any value by clicking the <code>Not Missing</code> button in this section of the page.</p> <p></p>"},{"location":"annotation_tool/#unlinking-columns","title":"Unlinking columns","text":"<p>Data table columns can also be unlinked (e.g. un-categorized) on the Annotation page in this section of the page labeled <code>Review the annotated columns</code>. This will stop the column from being annotated and have any annotations made for its values removed. The change is also reflected on the previous Categorization page.</p> <p></p> <p>Next page criteria: At least one annotation must be made from any of the categorized columns</p>"},{"location":"annotation_tool/#download-page","title":"Download page","text":"<p>Click the download annotated data button to download what we refer to an 'annotated' data dictionary that is a Neurobagel-enhanced BIDS-style data dictionary. This file will include any entries in the original data dictionary that you uploaded on the home page.</p> <p></p>"},{"location":"api/","title":"The Neurobagel API","text":"<p>The Neurobagel API, hosted at https://api.neurobagel.org/,  is a REST API that interfaces with Neurobagel's own running knowledge graph instance.  The API formulates SPARQL queries to the graph from a set of user-defined parameters and  processes returned query results into a user-friendly format.</p> <p>Neurobagel's query tool (https://query.neurobagel.org) queries the Neurobagel graph by sending requests to the API.  However, HTTP requests can also be sent directly to the Neurobagel API.  An independent local or institutional deployment of the API is also possible to interface with a local or restricted graph (see section on setting up a graph).</p>"},{"location":"api/#quickstart","title":"Quickstart","text":"<p>Queries of the knowledge graph can be submitted via direct requests to the API using the <code>https://api.neurobagel.org/query/</code> endpoint.  Specific query parameters are defined using key-value pairs in the URL following <code>/query/</code>.</p> <p>Example: \"I want to query for only female participants in the graph.\"</p> <p>The URL for such a query would be <code>https://api.neurobagel.org/query/?sex=snomed:248152002</code>, where <code>snomed:248152002</code> is a controlled term from the SNOMED CT vocabulary corresponding to female sex.</p>"},{"location":"api/#example-using-a-curl-request","title":"Example using a curl request","text":"<pre><code># To query for female participants in the graph\n\ncurl -X 'GET' \\\n  'http://localhost:8000/query/?sex=snomed:248152002' \\\n  -H 'accept: application/json'\n\n# or\ncurl -L http://localhost:8000/query/?sex=snomed:248152002\n</code></pre>"},{"location":"api/#using-the-interactive-api-docs","title":"Using the interactive API docs","text":"<p>Interactive documentation for the API (provided by Swagger UI) is available at https://api.neurobagel.org/docs and can also be used to run queries against the graph.</p> <p>Note</p> <p>For convenience, navigating to https://api.neurobagel.org/ in the browser will automatically redirect you to the docs.</p> <p>To send a request to the API from the docs interface, expand the <code>query</code> endpoint tab with the  icon to view the parameters that can be set,  and click \"Try it out\" and then \"Execute\" to execute a query.</p> <p>Note</p> <p>Due to limitations of Swagger UI in displaying very large HTTP response bodies,  queries with very few parameters sent using the interactive docs UI may be very slow or time out.  If this is the case, try using a <code>curl</code> request or the query tool instead.</p>"},{"location":"cite/","title":"Citing Neurobagel","text":"<p>If you use Neurobagel in your research, we recommend citing the Zenodo DOI associated with the Neurobagel tool and version you used.</p> Tool Zenodo reference Annotation tool CLI API Query tool <p>Note</p> <p>If you used the query tool, we recommend also citing the API.</p>"},{"location":"cli/","title":"The Neurobagel CLI","text":"<p>The <code>bagel-cli</code> is a simple Python command-line tool to automatically parse and describe subject-level phenotypic and BIDS attributes in an annotated dataset for integration into the Neurobagel graph.</p>"},{"location":"cli/#installation","title":"Installation","text":""},{"location":"cli/#docker","title":"Docker","text":"<p>Option 1 (RECOMMENDED): Pull the Docker image for the CLI from DockerHub: <code>docker pull neurobagel/bagelcli</code></p> <p>Option 2: Clone the repository and build the Docker image locally: <pre><code>git clone https://github.com/neurobagel/bagel-cli.git\ncd bagel-cli\ndocker build -t bagel .\n</code></pre></p>"},{"location":"cli/#singularity","title":"Singularity","text":"<p>Build a Singularity image for <code>bagel-cli</code> using the DockerHub image: <code>singularity pull bagel.sif docker://neurobagel/bagelcli</code></p>"},{"location":"cli/#running-the-cli","title":"Running the CLI","text":"<p>CLI commands can be accessed using the Docker/Singularity image.</p> <p>Note</p> <p>The Docker examples below assume that you are using the official Neurobagel Docker Hub image for the CLI.  If you have instead locally built an image, replace <code>neurobagel/bagelcli</code> in commands with your built image tag.</p>"},{"location":"cli/#input-files","title":"Input files","text":"<p>To run the CLI on a dataset you have annotated, you will need:</p> <ul> <li>A phenotypic TSV</li> <li>A corresponding phenotypic JSON data dictionary</li> <li>(Optional) The imaging dataset in BIDS format, if subjects have imaging data available (1)</li> </ul> <ol> <li>A valid BIDS dataset is needed for the CLI to automatically generate harmonized subject-level imaging metadata alongside harmonized phenotypic attributes.</li> </ol>"},{"location":"cli/#to-view-the-available-cli-commands","title":"To view the available CLI commands","text":"DockerSingularity <pre><code># Note: this is a shorthand for `docker run --rm neurobagel/bagelcli --help`\ndocker run --rm neurobagel/bagelcli\n</code></pre> <pre><code># Note: this is a shorthand for `singularity run bagel.sif --help`\nsingularity run bagel.sif\n</code></pre> <p>To view the command-line arguments for a specific command:</p> DockerSingularity <pre><code>docker run --rm neurobagel/bagelcli &lt;command-name&gt; --help\n</code></pre> <pre><code>singularity run bagel.sif &lt;command-name&gt; --help\n</code></pre>"},{"location":"cli/#to-run-the-cli-on-data","title":"To run the CLI on data","text":"<ol> <li><code>cd</code> into your local directory containing (1) your phenotypic .tsv file, (2) Neurobagel-annotated data dictionary, and (3) BIDS directory (if available). </li> <li>Run a <code>bagel-cli</code> container and include your CLI command and arguments at the end in the following format:</li> </ol> DockerSingularity <pre><code>docker run --rm --volume=$PWD:$PWD -w $PWD neurobagel/bagelcli &lt;CLI command here&gt;\n</code></pre> <pre><code>singularity run --no-home --bind $PWD --pwd $PWD /path/to/bagel.sif &lt;CLI command here&gt;\n</code></pre> <p>In the above command, <code>--volume=$PWD:$PWD -w $PWD</code> (or <code>--bind $PWD --pwd $PWD</code> for Singularity) mounts your current working directory (containing all inputs for the CLI) at the same path inside the container, and also sets the container's working directory to the mounted path (so it matches your location on your host machine). This allows you to pass paths to the containerized CLI which are composed the same way as on your local machine. (And both absolute paths and relative top-down paths from your working directory will work!)</p>"},{"location":"cli/#example","title":"Example","text":"<p>If your dataset lives in <code>/home/data/Dataset1</code>:</p> <pre><code>home/\n\u2514\u2500\u2500 data/\n    \u2514\u2500\u2500 Dataset1/\n        \u251c\u2500\u2500 neurobagel/\n        \u2502   \u251c\u2500\u2500 Dataset1_pheno.tsv\n        \u2502   \u2514\u2500\u2500 Dataset1_pheno.json\n        \u2514\u2500\u2500 bids/\n            \u251c\u2500\u2500 sub-01\n            \u251c\u2500\u2500 sub-02\n            \u2514\u2500\u2500 ...\n</code></pre> <p>You could run the CLI as follows:</p> DockerSingularity <pre><code>cd /home/data/Dataset1\n\n# 1. Generate phenotypic subject-level graph data (pheno.jsonld)\ndocker run --rm --volume=$PWD:$PWD -w $PWD neurobagel/bagelcli pheno \\\n    --pheno \"neurobagel/Dataset1_pheno.tsv\" \\\n    --dictionary \"neurobagel/Dataset1_pheno.json\" \\\n    --name \"My dataset 1\" \\\n    --output \"neurobagel/Dataset1_pheno.jsonld\"\n\n# 2. Add BIDS data to pheno.jsonld generated by step 1\ndocker run --rm --volume=$PWD:$PWD -w $PWD neurobagel/bagelcli bids \\\n    --jsonld-path \"neurobagel/pheno.jsonld\" \\\n    --bids-dir \"bids\" \\\n    --output \"neurobagel/Dataset1_pheno_bids.jsonld\"\n</code></pre> <pre><code>cd /home/data/Dataset1\n\n# 1. Generate phenotypic subject-level graph data (pheno.jsonld)\nsingularity run --no-home --bind $PWD --pwd $PWD bagel.sif pheno \\\n    --pheno \"neurobagel/Dataset1_pheno.tsv\" \\\n    --dictionary \"neurobagel/Dataset1_pheno.json\" \\\n    --name \"My dataset 1\" \\\n    --output \"neurobagel/Dataset1_pheno.jsonld\"\n\n# 2. Add BIDS data to pheno.jsonld generated by step 1\nsingularity run --no-home --bind $PWD --pwd $PWD bagel.sif bids \\\n    --jsonld-path \"neurobagel/pheno.jsonld\" \\\n    --bids-dir \"bids\" \\\n    --output \"neurobagel/Dataset1_pheno_bids.jsonld\"\n</code></pre> <p>Note</p> <p>The <code>bids</code> command of the <code>bagel-cli</code> (step 2) currently can take upwards of several minutes for datasets greater than a few hundred subjects, due to the time needed for pyBIDS to read the dataset structure. Once the slow initial dataset reading step is complete, you should see the message: <pre><code>Parsing BIDS metadata to be merged with phenotypic annotations:\n...\n</code></pre></p>"},{"location":"cli/#development-environment","title":"Development environment","text":"<p>To set up a development environment, please run <pre><code>git clone https://github.com/neurobagel/bagel-cli.git\ncd bagel-cli\npip install -e '.[all]'\n</code></pre></p>"},{"location":"data_prep/","title":"Preparing the phenotypic data","text":"<p>To use the Neurobagel annotation tool,  please prepare the tabular data for your dataset as a single, tab-separated file (<code>.tsv</code>).</p> <p>Note</p> <p>In the Neurobagel context, tabular or phenotypic data for a dataset refers to any demographic, clinical/behavioural, cognitive, or other non-imaging-derived data of participants  which are typically stored in a tabular file format.</p>"},{"location":"data_prep/#general-requirements-for-the-phenotypic-tsv","title":"General requirements for the phenotypic TSV","text":"<ul> <li>The TSV must contain a minimum of two columns: at least one column must contain subject IDs,  and at least one column must describe demographic or other phenotypic information  (for variables currently modeled by Neurobagel, see the data dictionary section).</li> <li>If the dataset has a corresponding BIDS directory (i.e., imaging data),  at least one column in the TSV must contain subject IDs that match the names of BIDS subject subdirectories.  Further, the IDs in this column must be the same or a superset of the subject labels in the BIDS directory.  That is, Neurobagel does not currently allow for datasets where subjects have BIDS data but are not represented in the phenotypic TSV.</li> </ul>"},{"location":"data_prep/#accepted-forms-of-tabular-data","title":"Accepted forms of tabular data","text":"<p>Depending on your dataset, your tabular data may represent one or more of the following:</p>"},{"location":"data_prep/#a-bids-participantstsv-file","title":"A BIDS <code>participants.tsv</code> file","text":"<p>If you have a BIDS compliant <code>participants.tsv</code> that contains  all the demographic and clinical/behavioural information for participants,  you can annotate this file with Neurobagel's annotation tool to create a data dictionary for the file.</p> <p>Example TSV:</p> participant_id age sex tools sub-01 22 female WASI-2 sub-02 28 male Stroop ..."},{"location":"data_prep/#a-longitudinal-data-file","title":"A longitudinal data file","text":"<p>If you have longitudinal tabular data (e.g. age collected at multiple sessions/visits),  then the information for all sessions should be combined into a single TSV.  Each row must describe a unique combination of subject and session.</p> <p>Example TSV:</p> participant_id session_id age tools sub-01 ses-01 22 WASI-2 sub-01 ses-02 23 sub-02 ses-01 28 Stroop ... <p>Tip</p> <p>A <code>participants.tsv</code> file with multiple sessions is not BIDS compliant.  If you want to store multi-session phenotypic data in a BIDS dataset,  you could do so in the <code>phenotype/</code> subdirectory  (see also the BIDS specification section on Longitudinal and multi-site studies).</p>"},{"location":"data_prep/#multiple-participant-or-session-identifier-columns","title":"Multiple participant or session identifier columns","text":"<p>In some cases, there may be a need for more than one set of IDs  for participants and/or sessions.</p> <p>For example, if a participant was first enrolled in a behavioural study with one type of ID,  and then later joined an imaging study under a different ID. In this case, both types of participant IDs should be recorded in the tabular file.</p> <p>The only requirement is that the combination of all ID values for a row is unique.</p> <p>Example invalid TSV:</p> participant_id alternative_participant_id ... sub-01 SID-1234 sub-01 SID-2222 sub-02 SID-1234 <p>The same rules apply when multiple session IDs are present.</p> <p>Example valid TSV:</p> participant_id alt_participant_id session_id alt_session_id age ... sub-01 SID-1234 ses-01 visit-1 22 sub-01 SID-1234 ses-02 visit-2 23 sub-02 SID-2222 ses-01 visit-1 28 ..."},{"location":"dictionaries/","title":"Neurobagel data dictionaries","text":""},{"location":"dictionaries/#overview","title":"Overview","text":"<p>When you annotate a phenotypic TSV using the Neurobagel annotation tool  (see also the section on the annotation tool), your annotations are automatically stored in a JSON data dictionary. A Neurobagel data dictionary essentially describes the meaning and properties of columns and column values using standardized vocabularies.</p> <p>Example</p> <p>A comprehensive example data dictionary containing all currently supported phenotypic attributes and annotations can be found here (corresponding phenotypic .tsv).</p> <p>Importantly, Neurobagel uses a structure for these data dictionaries that is compatible  with and expands on  BIDS <code>participant.json</code> data dictionaries. </p> <p>Info</p> <p>The specification for how a Neurobagel data dictionary is structured is also called a schema.  Because Neurobagel data dictionaries are stored as <code>.json</code> files, we use the <code>jsonschema</code> schema language  to write the specification.</p> <p>Neurobagel data dictionaries uniquely include an <code>Annotations</code> attribute  for each column entry to store user-provided semantic annotations.</p> <p>Here is an example BIDS data dictionary (<code>participants.json</code>):</p> <pre><code>{\n  \"age\": {\n    \"Description\": \"age of the participant\",\n    \"Units\": \"years\"\n  },\n  \"sex\": {\n    \"Description\": \"sex of the participant as reported by the participant\",\n    \"Levels\": {\n      \"M\": \"male\",\n      \"F\": \"female\"\n    }\n  }\n}\n</code></pre> <p>And here is the same data dictionary augmented with Neurobagel annotations:</p> <pre><code>{\n  \"age\": {\n    \"Description\": \"age of the participant\",\n    \"Units\": \"years\",\n    \"Annotations\": {\n      \"IsAbout\": {\n        \"TermURL\": \"http://neurobagel.org/vocab/Age\",\n        \"Label\": \"Age\"\n      },\n      \"Transformation\": {\n        \"TermURL\": \"http://neurobagel.org/vocab/int\",\n        \"Label\": \"Integer\"\n      }\n    }\n  },\n  \"sex\": {\n    \"Description\": \"sex of the participant as reported by the participant\",\n    \"Levels\": {\n      \"M\": \"male\",\n      \"F\": \"female\"\n    },\n    \"Annotations\": {\n      \"IsAbout\": {\n        \"TermURL\": \"http://neurobagel.org/vocab/Sex\",\n        \"Label\": \"Sex\"\n      },\n      \"Levels\": {\n        \"M\": {\n          \"TermURL\": \"http://purl.bioontology.org/ontology/SNOMEDCT/248153007\",\n          \"Label\": \"Male\"\n        },\n        \"F\": {\n          \"TermURL\": \"http://purl.bioontology.org/ontology/SNOMEDCT/248152002\",\n          \"Label\": \"Female\"\n        }\n      },\n      \"MissingValues\": [\n        \"\",\n        \" \"\n      ]\n    }\n  }\n}\n</code></pre> <p>A custom Neurobagel namespace (URI: <code>http://neurobagel.org/vocab/</code>) is currently used for controlled terms that represent attribute classes modelled by Neurobagel, such as <code>\"Age\"</code> and <code>\"Sex\"</code>, even though these terms may have equivalents in other vocabularies used for annotation. For example, the following terms from the Neurobagel annotations above are conceptually equivalent to terms from the SNOMED CT namespace:</p> Neurobagel namespace term Equivalent external controlled vocabulary term http://neurobagel.org/vocab/Age http://purl.bioontology.org/ontology/SNOMEDCT/397669002 http://neurobagel.org/vocab/Sex http://purl.bioontology.org/ontology/SNOMEDCT/184100006"},{"location":"dictionaries/#phenotypic-attributes","title":"Phenotypic attributes","text":"<p>The Neurobagel annotation tool generates a data dictionary entry for a given column  by augmenting the information recommended by BIDS with unambiguous semantic tags.</p> <p>Below we'll outline several example annotations using the following example <code>participants.tsv</code> file:</p> participant_id session_id group age sex updrs_1 updrs_2 sub-01 ses-01 PAT 25 M 2 sub-01 ses-02 PAT 26 M 3 5 sub-02 ses-01 CTL 28 F 1 1 sub-02 ses-02 CTL 29 F 1 1 <p>Controlled terms in the below examples are shortened using the RDF prefix/context syntax for json-ld:</p> <pre><code>{\n  \"@context\": {\n    \"nb\": \"http://neurobagel.org/vocab/\",\n    \"ncit\": \"http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl#\",\n    \"nidm\": \"http://purl.org/nidash/nidm#\",\n    \"snomed\": \"http://purl.bioontology.org/ontology/SNOMEDCT/\",\n    \"cogatlas\": \"https://www.cognitiveatlas.org/task/id/\"\n  }\n}\n</code></pre>"},{"location":"dictionaries/#participant-identifier","title":"Participant identifier","text":"<p>Term from the Neurobagel vocabulary.</p> <pre><code>{\n  \"participant_id\": {\n    \"Description\": \"A participant ID\",\n    \"Annotations\": {\n      \"IsAbout\": {\n        \"TermURL\": \"nb:ParticipantID\",\n        \"Label\": \"Subject Unique Identifier\"\n      },\n      \"Identifies\": \"participant\"\n    }\n  }\n}\n</code></pre> <p>Note</p> <p><code>participant_id</code> is a reserved name in BIDS and BIDS data dictionaries therefore typically don't annotate this column. Neurobagel supports multiple subject ID columns for situations where a study is using more than one ID scheme.</p> <p>Note</p> <p>The <code>Identifies</code> annotation key is currently required to validate annotations for columns about unique  observation identifiers  (e.g., participant or session IDs). The <code>\"Identifies\"</code> key should only be used for these columns and its value should be an informative string value describing the type/level of observation  identified. This required key is currently only used for validation and its value will not be processed by Neurobagel. (e.g., participant or session IDs), and should have an informative string value  describing the type/level of observation identified.</p>"},{"location":"dictionaries/#session-identifier","title":"Session identifier","text":"<p>Term from the Neurobagel vocabulary.</p> <pre><code>{\n  \"session_id\": {\n    \"Description\": \"A session ID\",\n    \"Annotations\": {\n      \"IsAbout\": {\n        \"TermURL\": \"nb:SessionID\",\n        \"Label\": \"Run Identifier\"\n      },\n      \"Identifies\": \"session\"\n    }\n  }\n}\n</code></pre> <p>Note</p> <p>Unlike the BIDS specification, Neurobagel supports a <code>participants.tsv</code> file with a <code>session_id</code> field.</p>"},{"location":"dictionaries/#diagnosis","title":"Diagnosis","text":"<p>Terms from the SNOMED-CT ontology for clinical diagnosis. Terms from the National Cancer Institute Thesaurus for healthy control status.</p> <pre><code>{\n  \"group\": {\n    \"Description\": \"Group variable\",\n    \"Levels\": {\n      \"PD\": \"Parkinson's patient\",\n      \"CTRL\": \"Control subject\",\n    },\n    \"Annotations\": {\n      \"IsAbout\": {\n        \"TermURL\": \"nb:Diagnosis\",\n        \"Label\": \"Diagnosis\"\n      },\n      \"Levels\": {\n        \"PD\": {\n          \"TermURL\": \"snomed:49049000\",\n          \"Label\": \"Parkinson's disease\"\n        },\n        \"CTRL\": {\n          \"TermURL\": \"ncit:C94342\",\n          \"Label\": \"Healthy Control\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>The <code>IsAbout</code> relation uses a term from the Neurobagel namespace because <code>\"Diagnosis\"</code> is a standardized term.</p> <p>Note</p> <p>Columns with categorical values (e.g., study groups, diagnoses, sex) require a <code>Levels</code> key in their Neurobagel annotation.  The Neurobagel \"Levels\" key is modeled after the BIDS \"Levels\" key for human readable descriptions.</p>"},{"location":"dictionaries/#sex","title":"Sex","text":"<p>Terms are from the SNOMED-CT ontology, which has controlled terms aligning with BIDS <code>participants.tsv</code> descriptions for sex.  Below are the SNOMED terms for the sex values allowed by BIDS: </p> Sex Controlled term Male http://purl.bioontology.org/ontology/SNOMEDCT/248153007 Female http://purl.bioontology.org/ontology/SNOMEDCT/248152002 Other http://purl.bioontology.org/ontology/SNOMEDCT/32570681000036106 <p>Here is what a sex annotation looks like in practice:</p> <pre><code>{\n  \"sex\": {\n    \"Description\": \"Sex variable\",\n    \"Levels\": {\n      \"M\": \"Male\",\n      \"F\": \"Female\"\n    },\n    \"Annotations\": {\n      \"IsAbout\": {\n        \"TermURL\": \"nb:Sex\",\n        \"Label\": \"Sex\"\n      },\n      \"Levels\": {\n        \"M\": {\n          \"TermURL\": \"snomed:248153007\",\n          \"Label\": \"Male\"\n        },\n        \"F\": {\n          \"TermURL\": \"snomed:248152002\",\n          \"Label\": \"Female\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>The <code>IsAbout</code> relation uses a Neurobagel scoped term for <code>\"Sex\"</code> because  this is a Neurobagel common data element.</p>"},{"location":"dictionaries/#age","title":"Age","text":"<p>Neurobagel has a common data element for <code>\"Age\"</code> describing a continuous column.  To ensure age values are represented as floats in Neurobagel graphs,  Neurobagel encodes the relevant \"heuristic\" describing the value format for a given age column.  This heuristic, stored in the <code>Transformation</code> annotation (required for continuous columns describing age),  maps internally to a specific transformation that is used to convert the values to floats.</p> <p>Possible heuristics: </p> TermURL Label nb:FromFloat float value nb:FromInt integer value nb:FromEuro european decimal value nb:FromBounded bounded value nb:FromISO8061 period of time defined according to the ISO8601 standard <pre><code>{\n  \"age\": {\n    \"Description\": \"Participant age\",\n    \"Annotations\": {\n      \"IsAbout\": {\n        \"TermURL\": \"nb:Age\",\n        \"Label\": \"Chronological age\"\n      },\n      \"Transformation\": {\n        \"TermURL\": \"nb:FromEuro\",\n        \"Label\": \"European value decimals\"\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"dictionaries/#assessment-tool","title":"Assessment tool","text":"<p>For assessment tools like cognitive tests or rating scales,  Neurobagel encodes whether the tool was successfully completed. Because assessment tools often have several subscales or items  that can be stored as separate columns in the tabular <code>participant.tsv</code> file, each assessment tool column gets a minimum of two annotations:</p> <ul> <li>one to classify that the column <code>IsAbout</code> the generic category of assessment tools</li> <li>one to classify that the column <code>IsPartOf</code> the specific assessment tool</li> </ul> <p>An optional additional annotation <code>MissingValues</code> can be used to specify value(s)  in an assessment tool column which represent that the participant is missing a value/response for that subscale, when instances of missing values are present (see also section Missing values).</p> <pre><code>{\n  \"updrs_1\": {\n    \"Description\": \"item 1 scores for UPDRS\",\n    \"Annotations\": {\n      \"IsAbout\": {\n        \"TermURL\": \"nb:Assessment\",\n        \"Label\": \"Assessment tool\"\n      },\n      \"IsPartOf\": {\n        \"TermURL\": \"cogatlas:tsk_4a57abb949ece\",\n        \"Label\": \"Unified Parkinson's Disease Rating Scale\"\n      }\n    }\n  },\n  \"updrs_2\": {\n    \"Description\": \"item 2 scores for UPDRS\",\n    \"Annotations\": {\n      \"IsAbout\": {\n        \"TermURL\": \"nb:Assessment\",\n        \"Label\": \"Assessment tool\"\n      },\n      \"IsPartOf\": {\n        \"TermURL\": \"cogatlas:tsk_4a57abb949ece\",\n        \"Label\": \"Unified Parkinson's Disease Rating Scale\"\n      },\n      \"MissingValues\": [\"\"]\n    }\n  }\n}\n</code></pre> <p>To determine whether a specific assessment tool is available for a given participant, we then combine all of the columns that were classified as <code>PartOf</code> that specific tool and then apply a simple <code>all()</code> heuristic to check that none of the columns contain any <code>MissingValues</code>.</p> <p>For the above example, this would be:</p> particpant_id updrs_1 updrs_2 sub-01 2 sub-02 1 1 <p>Therefore: </p> particpant_id updrs_available sub-01 False sub-02 True"},{"location":"dictionaries/#missing-values","title":"Missing values","text":"<p>Missing values are allowed for any phenotypic variable (column) that does not describe a participant or session identifier (e.g., columns like <code>participant_id</code> or <code>session_id</code>).  In a Neurobagel data dictionary, missing values for a given column are listed under the <code>\"MissingValues\"</code> annotation for the column (see the Assessment tool section or the comprehensive example data dictionary for examples).</p>"},{"location":"getting_help/","title":"Getting Help","text":"<p>For a bug or feature request, feel free to open an issue in the repository for the relevant tool in the Neurobagel GitHub organization.</p> <p>If you are using one of our web tools (https://query.neurobagel.org or https://query.neurobagel.org), you can also submit suggestions and bugs using the feedback widget directly on the site.</p> <p>For general questions about Neurobagel, please open an issue on Neurostars with the <code>neurobagel</code> tag: https://neurostars.org/tag/neurobagel.</p>"},{"location":"graph_data/","title":"Neurobagel graph data files","text":""},{"location":"graph_data/#overview","title":"Overview","text":"<p>Using the Neurobagel CLI (see also the section on the CLI),  a Neurobagel data dictionary (<code>.json</code>) for a dataset can be processed together with the corresponding tabular data (<code>.tsv</code>) and BIDS dataset (if available) to generate subject-level linked data that can be encoded in a knowledge graph.  The Neurobagel graph-ready data are stored in JSON-LD format (<code>.jsonld</code>),  and include a representation of each subject's harmonized phenotypic properties and imaging metadata.</p> <p>Another way to think about the difference between a Neurobagel data dictionary and a graph-ready <code>.jsonld</code> data file is this:  more than one dataset can theoretically have the same data dictionary (if the tabular data include the same columns and unique column values),  but the <code>.jsonld</code> file for each dataset is unique as long as the actual data of subjects differs across datasets.</p>"},{"location":"graph_data/#example-jsonld-files","title":"Example <code>.jsonld</code> files","text":"<p>Depending on whether a dataset annotated using Neurobagel includes BIDS imaging data,  the <code>.jsonld</code> data for the dataset may or may not include imaging metadata of subjects (extracted automatically with the CLI).</p> <ul> <li>Example valid <code>.jsonld</code> containing only phenotypic data</li> <li>Example valid <code>.jsonld</code> containing both phenotypic and BIDS data</li> </ul> More info on example dataset <p>The above <code>.jsonld</code> files represent an example dataset used for testing which includes the following:</p> Data Link Phenotypic TSV Neurobagel data dictionary BIDS dataset"},{"location":"infrastructure/","title":"SysAdmin","text":"<p>These instructions are for a sysadmin looking to deploy Neurobagel locally in an institute or lab.  A local neurobagel deployment includes the neurobagel API,  a graph backend to store the harmonized metadata,  and optionally a locally hosted graphical query interface.</p> <p></p> <p>Neurobagel uses RDF-triple stores as graph backends. Because RDF is an W3C open standard,  any RDF store can be theoretically used as a backend. We have tested the following options:</p> StardoggraphDB <p>Stardog  is a very performant RDF store with a large number of extensions. However, it has a very restrictive license. We therefore do not recommend Stardog for most deployments or testing.</p> <p>graphDB  offers a perpetual free license that should be sufficient for many smaller deployments or testing deployments. </p> <p>Note</p> <p>RDF stores are relatively niche applications for very large data applications, so most implementations are commercial.</p>"},{"location":"infrastructure/#get-a-license-for-the-graph-backend","title":"Get a license for the graph backend","text":"StardoggraphDB <p>Stardog has a free, annually renewable license for academic use. In order to make a separate deployment of Neurobagel,  you should therefore first request your own Stardog license. You can request a Stardog license here:</p> <p>https://www.stardog.com/license-request/</p> <p>Don't pick the wrong license</p> <p>Stardog is a company that offers their graph store solutions both as a self-hosted, downloadable tool (what we want) and as a cloud hosted subscription model (what we do not want). Both tiers offer free access and the website has a tendency to steer you towards the cloud offering. Make sure you request a license key for Stardog.</p> <p></p> <p>The Stardog license is typically automatically granted via email in 24 hours. </p> <p>The license you receive will be a downloadable file.  It is valid for one year and for a major version of Stardog. You will need to download the license in a place that is accessible to your new Stardog instance when it is launched (see below).</p> <p>graphDB creates a free perpetual license automatically when you don't explicitly provide a license. The free edition mostly offers the same features  as the paid versions,  but restricts the number of concurrent operations on the graph to 2. </p> <p>We recommend using graphDB if these restrictions are not a blocker.</p>"},{"location":"infrastructure/#launch-the-api-and-graph-stack","title":"Launch the API and graph stack","text":"<p>We recommend launching the API and your graph backend instance using <code>docker compose</code>. The below steps are distilled from these instructions.</p>"},{"location":"infrastructure/#clone-the-api-repo","title":"Clone the API repo","text":"<pre><code>git clone https://github.com/neurobagel/api.git\n</code></pre>"},{"location":"infrastructure/#set-the-environment-variables","title":"Set the environment variables","text":"<p>Create a <code>.env</code> file in the root of the repository to house the environment variables used by the API-graph network.</p> <p>The <code>neurobagel/api</code> repo contains a <code>.template-env</code> to get you started. Copy and rename this file to <code>.env</code> and then edit it as needed.</p> <p>Below are all the possible Neurobagel environment variables that can be set in <code>.env</code>.</p> Environment variable Required in .env? Description Default value if not set Relevant installation mode(s) <code>NB_GRAPH_USERNAME</code> Yes Username to access Stardog graph database that API will communicate with - Docker, Python <code>NB_GRAPH_PASSWORD</code> Yes Password to access Stardog graph database that API will communicate with - Docker, Python <code>NB_GRAPH_ADDRESS</code> No IP address for the graph database (or container name, if graph is hosted locally) <code>206.12.99.17 (graph)</code> ** Docker, Python <code>NB_GRAPH_DB</code> No Name of graph database endpoint to query (e.g., for a Stardog database, this will take the format of <code>{database_name}/query</code>) <code>test_data/query</code> Docker, Python <code>NB_RETURN_AGG</code> No Whether to return only dataset-level query results (including data locations) and exclude subject-level attributes. One of [true, false] <code>true</code> Docker, Python <code>NB_API_TAG</code> No Docker image tag for the API <code>latest</code> Docker <code>NB_API_PORT_HOST</code> No Port number on the host machine to map the API container port to <code>8000</code> Docker <code>NB_API_PORT</code> No Port number on which to run the API <code>8000</code> Docker, Python <code>NB_API_ALLOWED_ORIGINS</code> Yes, if using a frontend query tool \u2021 Origins allowed to make cross-origin resource sharing requests. Multiple origins must be separated with spaces in a single string enclosed in quotes. See \u2021 for more info <code>\"\"</code> Docker, Python <code>NB_GRAPH_IMG</code> No Graph server Docker image <code>stardog/stardog:8.2.2-java11-preview</code> Docker <code>NB_GRAPH_ROOT_HOST</code> No Path to directory containing a Stardog license file on the host machine <code>~/stardog-home</code> Docker <code>NB_GRAPH_ROOT_CONT</code> No Path to directory for graph databases in the graph server container <code>/var/opt/stardog</code> * Docker <code>NB_GRAPH_PORT_HOST</code> No Port number on the host machine to map the graph server container port to <code>5820</code> Docker, Python <code>NB_GRAPH_PORT</code> No Port number used by the graph server container <code>5820</code> * Docker <code>NB_QUERY_TAG</code> No Docker image tag for the query tool <code>latest</code> Docker <code>NB_QUERY_PORT_HOST</code> No Port number used by the <code>query_tool</code> on the host machine <code>3000</code> Docker <code>API_QUERY_URL</code> Yes, unless default is correct URL of the API that the query tool will send its requests to. The port number in the URL must correspond to <code>NB_API_PORT_HOST</code>. See also the query tool README. Must end in a forward slash <code>/</code>! <code>http://localhost:8000/</code> Docker StardoggraphDB <p>* These defaults are configured for a Stardog backend - you should not have to change them if you are running a Stardog backend.</p> <p>Your Stardog license file must be in the right directory</p> <p>Note that your Stardog license file must be in the directory specified by <code>NB_GRAPH_ROOT_HOST</code> (default <code>~/stardog-home</code>).</p> <p>* These values will have to be changed for your deployment from their default value:</p> <p>Change the following default values in your .env file for a graphDB deployment!</p> <pre><code>NB_GRAPH_IMG=ontotext/graphdb:10.3.1\nNB_GRAPH_ROOT_CONT=/opt/graphdb/home\nNB_GRAPH_PORT=7200\nNB_GRAPH_PORT_HOST=7200\nNB_GRAPH_DB=repositories/my_db  # NOTE: for graphDB, this value should always take the the format of: repositories/&lt;your_database_name&gt;\n</code></pre> <p>** <code>NB_GRAPH_ADDRESS</code> should not be changed from its default value (<code>graph</code>) when using docker compose as this corresponds to the preset container name of the graph database server within the docker compose network.</p> <p>\u2021 See section Using a graphical query tool to send API requests</p> <p>For a local deployment, we recommend to explicitly set at least the following variables in <code>.env</code> (note that <code>NB_GRAPH_USERNAME</code> and <code>NB_GRAPH_PASSWORD</code> must always be set):</p> <p><code>NB_GRAPH_USERNAME</code> <code>NB_GRAPH_PASSWORD</code> <code>NB_GRAPH_DB</code> <code>NB_GRAPH_IMG</code> <code>NB_RETURN_AGG</code> <code>NB_API_ALLOWED_ORIGINS</code></p> Ensure that shell variables do not clash with <code>.env</code> file <p>If the shell you run <code>docker compose</code> from already has any  shell variable of the same name set,  the shell variable will take precedence over the configuration of <code>.env</code>! In this case, make sure to <code>unset</code> the local variable first.</p> <p>For more information, see Docker's environment variable precedence.</p>"},{"location":"infrastructure/#a-note-on-using-a-graphical-query-tool-to-send-api-requests","title":"A note on using a graphical query tool to send API requests","text":"<p>The <code>NB_API_ALLOWED_ORIGINS</code> variable defaults to an empty string (<code>\"\"</code>) when unset, meaning that your deployed API will only be accessible via direct <code>curl</code> requests to the URL where the API is hosted (see this section for an example <code>curl</code> request).</p> <p>However, in many cases you may want to make the API accessible by a frontend tool such as our browser query tool. To do so, you must explicitly specify the origin(s) for the frontend using <code>NB_API_ALLOWED_ORIGINS</code> in <code>.env</code>. </p> <p>For example, the <code>.template-env</code> file in the Neurobagel API repo assumes you want to allow API requests from a query tool hosted at a specific port on <code>localhost</code> (see the Docker Compose section).</p> <p>Other examples: <pre><code># ---- .env file ----\n\n# do not allow requests from any frontend origins\nNB_API_ALLOWED_ORIGINS=\"\"  # this is the default value that will also be set if the variable is excluded from the .env file\n\n# allow requests from only one origin\nNB_API_ALLOWED_ORIGINS=\"https://query.neurobagel.org\"\n\n# allow requests from 3 different origins\nNB_API_ALLOWED_ORIGINS=\"https://query.neurobagel.org https://localhost:3000 http://localhost:3000\"\n\n# allow requests from any origin - use with caution\nNB_API_ALLOWED_ORIGINS=\"*\"\n</code></pre></p> <p>A note for more technical users: If you have configured an NGINX reverse proxy (or proxy requests to the remote origin) to serve both the API and the query tool from the same origin, you can skip the step of enabling CORS for the API.  For an example, see https://docs.nginx.com/nginx/admin-guide/web-server/reverse-proxy/.</p>"},{"location":"infrastructure/#docker-compose","title":"Docker Compose","text":"<p>To spin up the API and graph backend containers using Docker Compose,  ensure that both docker and docker compose are installed.</p> <p>Run the following in the repository root (where the <code>docker-compose.yml</code> file is) to launch the containers:</p> <p>Tip</p> <p>Double check that any environment variables you have customized in <code>.env</code> are resolved with your expected values using the command <code>docker compose config</code>.</p> <p><pre><code>docker compose up -d\n</code></pre> Or, if you want to ensure you always pull the latest Docker images first: <pre><code>docker compose pull &amp;&amp; docker compose up -d\n</code></pre></p> <p>By default, this will also deploy a local version of the Neurobagel graphical query tool. If using the default port mappings, you can reach your local query tool at http://localhost:3000 once it is running.</p>"},{"location":"infrastructure/#setup-for-the-first-run","title":"Setup for the first run","text":"<p>When you launch the graph backend for the first time, there are a couple of setup steps that need to be done.  These will not have to be repeated for subsequent starts.</p> <p>To interact with your graph backend,  you have two general options:</p> StardoggraphDB <ol> <li>Send HTTP request against the HTTP API of the Stardog graph instance (e.g. with <code>curl</code>). See https://stardog-union.github.io/http-docs/ for a full reference of API endpoints</li> <li>Use the free Stardog-Studio web app. See the Stardog documentation for instruction to deploy Stardog-Studio as a Docker container.</li> </ol> <p>Info</p> <p>Stardog-Studio is the most accessible way  of manually interacting with a Stardog instance.  Here we will focus instead on using the HTTP API for configuration, as this allows programmatic access. All of these steps can also be achieved via Stardog-Studio manually. Please refer to the  official docs to learn how.</p> <ol> <li>Send HTTP requests against the HTTP API of the graphDB backend  e.g. using <code>curl</code>. graphDB uses the RDF4J API specification.</li> <li>Use the graphDB web interface (called the workbench).  Once your local graphDB backend is running you can connect to it at http://localhost:8000</li> </ol> <p>Info</p> <p>Using the graphDB workbench is a more accessible way to manage the graphDB endpoint. The workbench is well documented on the graphDB website. Here we will focus instead on setting up graphDB with API calls,  that can be automated.</p>"},{"location":"infrastructure/#change-the-database-admin-password","title":"Change the database admin password","text":"StardoggraphDB <p>When you first launch Stardog,  a default <code>admin</code> user with superuser privilege will automatically be created for you. This <code>admin</code> user is meant to create other database users and modify their permissions. Do not use <code>admin</code> for read and write operations, instead use a regular database user.</p> <p>You should first change the password of the database <code>admin</code>:</p> <pre><code>curl -X PUT -i -u \"admin:admin\" http://localhost:5820/admin/users/admin/pwd \\\n--data '{\"password\": \"NewAdminPassword\"}'\n</code></pre> <p>When the API, graph, and query tool have been started and are running for the first time, you will have to do some first-run configuration.</p> <p>Setup security and users</p> <p>Also refer to the official graphDB documentation.</p> <p>First, change the password for the admin user that has been automatically created by graphDB:</p> <p><pre><code>curl -X PATCH --header 'Content-Type: application/json' http://localhost:7200/rest/security/users/admin -d '\n{\"password\": \"NewAdminPassword\"}'\n</code></pre> make sure to replace <code>\"NewAdminPassword\"</code> with your own, secure password.</p> <p>Next, enable graphDB security to only allow authenticated users access: <pre><code>curl -X POST --header 'Content-Type: application/json' -d true http://localhost:7200/rest/security\n</code></pre></p> <p>and confirm that this was successful: <pre><code>\u279c curl -X POST http://localhost:7200/rest/security                                                  \nUnauthorized (HTTP status 401)\n</code></pre></p>"},{"location":"infrastructure/#create-a-new-database-user","title":"Create a new database user","text":"<p>The <code>.env</code> file created as part of the <code>docker compose</code> setup instructions declares the <code>NB_GRAPH_USERNAME</code> and <code>NB_GRAPH_PASSWORD</code> for the database user. The API will send requests to the graph using these credentials. When you launch Stardog for the first time,  we have to create a new database user:</p> StardoggraphDB <pre><code>curl -X POST -i -u \"admin:NewAdminPassword\" http://localhost:5820/admin/users \\\n-H 'Content-Type: application/json' \\\n--data '{\n    \"username\": \"DBUSER\",\n    \"password\": [\n        \"DBPASSWORD\"\n    ]\n}'\n</code></pre> <p>Confirm that the new user exists:</p> <pre><code>curl -u \"admin:NewAdminPassword\" http://localhost:5820/admin/users\n</code></pre> <pre><code>curl -X POST --header 'Content-Type: application/json' -u \"admin:NewAdminPassword\" -d '\n{\n\"username\": \"DBUSER\",\n\"password\": \"DBPASSWORD\"\n}' http://localhost:7200/rest/security/users/DBUSER\n</code></pre> <p>Note</p> <p>Make sure to use the exact <code>NB_GRAPH_USERNAME</code> and <code>NB_GRAPH_PASSWORD</code> you defined in the <code>.env</code> file when creating the new database user. Otherwise the API will not have the correct permission to query the graph.</p>"},{"location":"infrastructure/#create-new-database","title":"Create new database","text":"<p>When you first launch graph store, there are no graph databases. You have to create a new one to store your metadata.</p> <p>If you have defined a custom <code>NB_GRAPH_DB</code> name in the <code>.env</code> file, make sure to create a database with a matching name. By default the API will query a graph database with a name of <code>test_data</code>.</p> StardoggraphDB <pre><code>curl -X POST -i -u \"admin:NewAdminPassword\" http://localhost:5820/admin/databases \\\n--form 'root=\"{\\\"dbname\\\":\\\"test_data\\\"}\"'\n</code></pre> <p>Now we need to give our new database user read and write permission for  this database:</p> <pre><code>curl -X PUT -i -u \"admin:NewAdminPassword\" http://localhost:5820/admin/permissions/user/DBUSER \\\n-H 'Content-Type: application/json' \\\n--data '{\n    \"action\": \"ALL\",\n    \"resource_type\": \"DB\",\n    \"resource\": [\n        \"test_data\"\n    ]\n}'\n</code></pre> Finer permission control is also possible <p>For simplicity's sake, here we give <code>\"ALL\"</code> permission to the new database user. The Stardog API provide more fine grained permission control. See the official API documentation.</p> <p>In graphDB, graph databases are called resources. To create a new one, you will also have to prepare a <code>data-config.ttl</code> file that contains the settings for the resource you will create (see the graphDB docs).</p> <p>make sure to that the value for <code>rep:repositoryID</code> in the <code>data-configl.ttl</code> file matches the value of <code>NB_GRAPH_DB</code> in your <code>.env</code> file.  For example, if <code>NB_GRAPH_DB=my_db</code>, then <code>rep:repositoryID \"my_db\" ;</code>.</p> <p>You can use this example file and save it as <code>data-config.ttl</code> locally:</p> <pre><code>#\n# RDF4J configuration template for a GraphDB repository\n#\n@prefix rdfs: &lt;http://www.w3.org/2000/01/rdf-schema#&gt;.\n@prefix rep: &lt;http://www.openrdf.org/config/repository#&gt;.\n@prefix sr: &lt;http://www.openrdf.org/config/repository/sail#&gt;.\n@prefix sail: &lt;http://www.openrdf.org/config/sail#&gt;.\n@prefix graphdb: &lt;http://www.ontotext.com/config/graphdb#&gt;.\n\n[] a rep:Repository ;\n    rep:repositoryID \"my_db\" ;\n    rdfs:label \"\" ;\n    rep:repositoryImpl [\n        rep:repositoryType \"graphdb:SailRepository\" ;\n        sr:sailImpl [\n            sail:sailType \"graphdb:Sail\" ;\n\n            graphdb:read-only \"false\" ;\n\n            # Inference and Validation\n            graphdb:ruleset \"rdfsplus-optimized\" ;\n            graphdb:disable-sameAs \"true\" ;\n            graphdb:check-for-inconsistencies \"false\" ;\n\n            # Indexing\n            graphdb:entity-id-size \"32\" ;\n            graphdb:enable-context-index \"false\" ;\n            graphdb:enablePredicateList \"true\" ;\n            graphdb:enable-fts-index \"false\" ;\n            graphdb:fts-indexes (\"default\" \"iri\") ;\n            graphdb:fts-string-literals-index \"default\" ;\n            graphdb:fts-iris-index \"none\" ;\n\n            # Queries and Updates\n            graphdb:query-timeout \"0\" ;\n            graphdb:throw-QueryEvaluationException-on-timeout \"false\" ;\n            graphdb:query-limit-results \"0\" ;\n\n            # Settable in the file but otherwise hidden in the UI and in the RDF4J console\n            graphdb:base-URL \"http://example.org/owlim#\" ;\n            graphdb:defaultNS \"\" ;\n            graphdb:imports \"\" ;\n            graphdb:repository-type \"file-repository\" ;\n            graphdb:storage-folder \"storage\" ;\n            graphdb:entity-index-size \"10000000\" ;\n            graphdb:in-memory-literal-properties \"true\" ;\n            graphdb:enable-literal-index \"true\" ;\n        ]\n    ].\n</code></pre> <p>Then you can create a new graph db with the following command (replace \"my_db\" as needed):</p> <pre><code>curl -X PUT -u \"admin:NewAdminPassword\" http://localhost:7200/repositories/my_db --data-binary \"@data-config.ttl\" -H \"Content-Type: application/x-turtle\"\n</code></pre> <p>and add give our user access permission to the new resource:</p> <pre><code>curl -X PUT --header 'Content-Type: application/json' -d '\n{\"grantedAuthorities\": [\"WRITE_REPO_my_db\",\"READ_REPO_my_db\"]}'  http://localhost:7200/rest/security/users/DBUSER -u \"admin:NewAdminPassword\"\n</code></pre> <ul> <li><code>\"WRITE_REPO_my_db\"</code>: Grants write permission.</li> <li><code>\"READ_REPO_my_db\"</code>: Grants read permission.</li> </ul> <p>Note</p> <p>make sure you replace <code>my_db</code> with the name of the graph db you  have just created. </p>"},{"location":"infrastructure/#uploading-data-to-the-graph","title":"Uploading data to the graph","text":"<p>The <code>neurobagel/api</code> repo contains a helper script <code>add_data_to_graph.sh</code> for automatically uploading all JSONLD files (i.e., graph-ready data) in a directory to a specific graph database, with the option to clear the existing data in the database first. Each <code>.jsonld</code> file is expected to correspond to a single dataset.</p> <p>To view all the command line arguments for add_data_to_graph.sh: <pre><code>./add_data_to_graph.sh --help\n</code></pre></p> If you prefer to directly use <code>curl</code> requests to modify the graph database instead of the helper script <p>Add a single dataset to the graph database (example) <pre><code>curl -u \"&lt;USERNAME&gt;:&lt;PASSWORD&gt;\" -i -X POST http://localhost:5820/&lt;DATABASE_NAME&gt; \\\n    -H \"Content-Type: application/ld+json\" \\\n    --data-binary @&lt;DATASET_NAME&gt;.jsonld\n</code></pre></p> <p>Clear all data in the graph database (example) <pre><code>curl -u \"&lt;USERNAME&gt;:&lt;PASSWORD&gt;\" -X POST http://localhost:5820/&lt;DATABASE_NAME&gt;/update \\\n    -H \"Content-Type: application/sparql-update\" \\\n    --data-binary \"DELETE { ?s ?p ?o } WHERE { ?s ?p ?o }\"\n</code></pre></p>"},{"location":"infrastructure/#uploading-example-neurobagel-data","title":"Uploading example Neurobagel data","text":"<p>In order to test that the graph setup steps worked correctly, we can add some example graph-ready data to the new graph database.</p> <p>First, clone the <code>neurobagel_examples</code> repository:</p> <pre><code>git clone https://github.com/neurobagel/neurobagel_examples.git\n</code></pre> <p>Next, upload the <code>.jsonld</code> file in the directory <code>neurobagel_examples/data-upload/pheno-bids-output</code> to the database we created above, using <code>add_data_to_graph.sh</code>:</p> <p>Info</p> <p>Normally you would create the graph-ready files by first annotating the phenotypic information of a BIDS dataset with the  Neurobagel annotator, and then parsing the annotated BIDS dataset with the Neurobagel CLI.</p> StardoggraphDB <pre><code>./add_data_to_graph.sh PATH/TO/neurobagel_examples/data-upload/pheno-bids-output \\ \n  localhost:5820 test_data DBUSER DBPASSWORD \\\n  --clear-data\n</code></pre> <pre><code>./add_data_to_graph.sh PATH/TO/neurobagel_examples/data-upload/pheno-bids-output \\ \n  localhost:7200 repositories/my_db/statements DBUSER DBPASSWORD \\\n  --clear-data\n</code></pre> <p>Note: Here we added the <code>--clear-data</code> flag to remove any existing data in the database (if the database is empty, the flag has no effect). You can choose to omit the flag or explicitly specify <code>--no-clear-data</code> (default behaviour) to skip this step.</p>"},{"location":"infrastructure/#updating-a-dataset-in-the-graph-database","title":"Updating a dataset in the graph database","text":"<p>If the raw data for a previously harmonized dataset (i.e., already has a corresponding JSONLD which is in the graph) has been updated, a new JSONLD file must first be generated for that dataset. To push the update to the corresponding graph database, our current recommended approach is to simply clear the database and re-upload all existing datasets, including the new JSONLD file for the updated dataset.</p> <p>To do this, rerun <code>add_data_to_graph.sh</code> on the directory containing the JSONLD files currently in the graph database, including the replaced JSONLD file for the dataset that has been updated. Make sure to include the <code>--clear-data</code> flag when running the script so that the database is cleared first.</p>"},{"location":"infrastructure/#where-to-store-neurobagel-graph-ready-data","title":"Where to store Neurobagel graph-ready data","text":"<p>To allow easy (re-)uploading of datasets when needed, we recommend having a shared directory in your data filesystem/server for storing Neurobagel graph-ready JSONLD files created for datasets at your institute or lab.  This directory can be called anything you like, but we recommend an explicit name such as <code>neurobagel_jsonld_datasets</code> to distinguish it from the actual raw data files or Neurobagel data dictionaries. Each <code>.jsonld</code> in the directory should include the name of the dataset in the filename.</p>"},{"location":"infrastructure/#test-the-new-deployment","title":"Test the new deployment","text":"<p>You can run a test query against the API via a <code>curl</code> request in your terminal:</p> <pre><code>curl -X 'GET' \\\n  'http://localhost:8000/query/' \\\n  -H 'accept: application/json'\n\n# or\ncurl -L http://localhost:8000/query/\n</code></pre> <p>Or, you can directly use the interactive documentation of the API (provided by Swagger UI)  by navigating to http://localhost:8000/docs in your browser.  To test the API from the docs interface, expand the <code>query</code> endpoint tab with the  icon to view the parameters that can be set,  and click \"Try it out\" and then \"Execute\" to execute a query.</p> <p>Note</p> <p>For very large databases, requests to the API using the interactive docs UI may be very slow or time out.  If this prevents test queries from succeeding, try setting more parameters to enable an example response from the graph, or use a <code>curl</code> request instead.</p>"},{"location":"overview/","title":"Ecosystem","text":"<p>The Neurobagel ecosystem comprises four primary tools:</p> <ul> <li>The annotation tool  (annotate.neurobagel.org)<ul> <li>to create harmonized annotations of phenotypic variables</li> <li>intended for use by researchers and domain experts</li> <li>static site, deployed on GitHub Pages</li> </ul> </li> <li>The command-line interface <ul> <li>to extract subject-specific metadata from annotated phenotypic and BIDS data</li> <li>intended for data managers to create graph-ready harmonized data</li> </ul> </li> <li>The knowledge graph store and API  (api.neurobagel.org)<ul> <li>to store and query extracted metadata using RDF and the SPARQL query language</li> <li>intended for research/data platform owners and for isolated deployments</li> </ul> </li> <li>The query tool  (query.neurobagel.org)<ul> <li>to search across datasets and obtain metadata for subjects based on harmonized subject-level attributes</li> <li>intended to help researchers and scientific data users find cohorts</li> <li>static site, deployed on GitHub Pages</li> </ul> </li> </ul> <p>You can also find official Docker images for our containerized tools on the Neurobagel Docker Hub profile.</p> Todo <p>Add Neurobagel figure for overview.</p>"},{"location":"overview/#getting-started","title":"Getting started","text":"<p>Want to annotate an exiting dataset using Neurobagel? See Annotating a dataset.</p> <p>Want to deploy the Neurobagel software stack at your local research institute? See Setting up a graph.</p> <p>Want to search for participants in the Neurobagel graph? See Running cohort queries.</p>"},{"location":"query_tool/","title":"The Neurobagel Query Tool","text":"<p>Neurobagel's query tool is a web interface for searching across a Neurobagel graph based on various subject clinical-demographic and imaging parameters.</p> <p>The query tool is a Vue application, developed in JavaScript using a variety of tools including Nuxt, Cypress, and BootstrapVue.</p>"},{"location":"query_tool/#quickstart","title":"Quickstart","text":"<p>The query tool is hosted at https://query.neurobagel.org/ and interfaces with Neurobagel API.</p> <p>NOTE: Currently, to access the query tool, you must be connected to the McGill network.</p>"},{"location":"query_tool/#local-installation","title":"Local Installation","text":""},{"location":"query_tool/#install-dependencies","title":"Install Dependencies","text":"<p>To run the query tool, you'll need node package manager (npm) and Node.js. You can find the instructions on installing npm and node in the official documentation.</p> <p>Once you have npm and node installed, you'll need to install the dependencies outlined in the package.json file. You can do so by running the following command:</p> <pre><code>npm install\n</code></pre> <p>use node v16.x LTS!</p> <p>  The query tool is built with the Nuxt framework and currently depends on Nuxt2.  Nuxt2 does not support node versions beyond the v16 LTS (see e.g. this Github issue). If you want to run the tool locally, make sure you are using node v16.x. A good way to manage different node versions is to use the node version manager tool.</p>"},{"location":"query_tool/#set-the-environment-variables","title":"Set the Environment Variables","text":"<p>You'll need to set the <code>API_QUERY_URL</code> environment variable required to run the query tool. <code>API_QUERY_URL</code> is the Neurobagel API URL that the query tool uses to send requests to for results. The query tool utilizes nuxt dotenv module for managing environment variables. </p> <p>To set environment variables, create an <code>.env</code> file in the root directory and add the environment variables there. If you're running the API locally on your machine (following the instructions here), your <code>.env</code> file would look something like this:</p> <pre><code>API_QUERY_URL=http://localhost:8000/\n</code></pre> <p>if you're using the remote api, your <code>.env</code> file would look something like this:</p> <pre><code>API_QUERY_URL=https://api.neurobagel.org/\n</code></pre>"},{"location":"query_tool/#launch-the-query-tool","title":"Launch the Query Tool","text":"<p>To launch the tool in developer mode run the following command:</p> <pre><code>npm run dev\n</code></pre> <p>You can also build and then run the tool from the (production) build of the application by running the following command:</p> <pre><code>npm run build &amp;&amp; npm run start\n</code></pre> <p>You can verify the tool is running once you receive info messages from Nuxt regarding environment, rendering, and what port the tool is running on in your terminal.</p>"},{"location":"query_tool/#usage","title":"Usage","text":""},{"location":"query_tool/#running-a-query","title":"Running a query","text":"<p>To define a cohort, set your inclusion criteria using the following:</p> <ul> <li>Age: Minimum and/or maximum age (in years) of participant that should be included in the results.</li> <li>Sex: Sex of participant that should be included in the results.</li> <li>Diagnosis: Diagnosis of participant that should be included in the results</li> <li>Healthy control: Whether healthy participants should be included in the results. Once healthy control checkbox is selected, diagnosis field will be disabled since a participant cannot be both a healthy control and have a diagnosis.</li> <li>Minimum number of sessions: Minimum number of imaging sessions that participant should have to be included in the results.</li> <li>Assessment tool: Non-imaging assessment completed by participant that should be included in the results.</li> <li>Modality: Imaging modality of participant scans that should be included in the results.</li> </ul> <p>Once you've defined your criteria, submit them as a query and the query tool will display the results.</p>"},{"location":"query_tool/#downloading-query-results","title":"Downloading query results","text":"<p>For a given query, the query tool offers two kinds of TSV files for results that users can download.  At least one dataset matching the query must be selected in the interface in order to download the query results.</p>"},{"location":"query_tool/#dataset-level-results","title":"Dataset-level results","text":"<p>The dataset-level results TSV describes the datasets that contain subjects matching the user's query. This TSV contains the following columns:</p> <ul> <li><code>DatasetID</code>: unique identifier (UUID) for dataset in the graph.  Note that this ID is not guaranteed to be persistent across versions of a graph/across graphs, but will always be identical across a pair of query tool result files.  This column can be used as the key to join the dataset-level and participant-level results TSVs for a given query result, if needed.</li> <li><code>DatasetName</code>: human readable name of the dataset</li> <li><code>PortalURI</code>: URL to a website or page about the dataset, if available</li> <li><code>NumMatchingSubjects</code>: (aggregate variable) number of subjects matching the query within the dataset</li> <li><code>AvailableImageModalites</code>: (aggregate variable) list of unique imaging modalities available for the dataset</li> </ul> <p>Example:</p> DatasetID DatasetName PortalURI NumMatchingSubjects AvailableImageModalities http://neurobagel.org/vocab/81e47a09-658d-48ea-b7c5-8d54820d038e BIDS synthetic https://github.com/bids-standard/bids-examples 5 http://purl.org/nidash/nidm#T1Weighted, http://purl.org/nidash/nidm#FlowWeighted"},{"location":"query_tool/#participant-level-results","title":"Participant-level results","text":"<p>The participant-level results TSV contains the available harmonized participant attributes for subject sessions matching the query in each (selected) matching dataset. Each row in the TSV corresponds to a single matching subject session.</p> <p>This TSV contains the following columns:</p> <ul> <li><code>DatasetID</code>: unique identifier (UUID) for dataset in the graph.  Note that this ID is not guaranteed to be persistent across versions of a graph/across graphs, but will always be identical across a pair of query tool result files.  This column can be used as the key to join the dataset-level and participant-level results TSVs for a given query result, if needed.</li> <li><code>SubjectID</code>: subject label</li> <li><code>Age</code>: subject age, if available</li> <li><code>Sex</code>: subject sex, if available</li> <li><code>Diagnosis</code>: list of diagnoses of subject, if available</li> <li><code>Assessment</code> : list of assessments completed by subject, if available</li> <li><code>SessionID</code>: session label</li> <li><code>SessionPath</code>: the path of the session directory relative to the dataset root (for datasets available through DataLad) or root of the filesystem where the dataset is stored</li> <li><code>NumSessions</code>: (aggregate variable) total number of available sessions for subject.  This number will be the same across rows corresponding to the same subject.</li> <li><code>Modality</code>: imaging modalities acquired in the session, if available</li> </ul> <p>Example:</p> DatasetID SubjectID Age Sex Diagnosis Assessment SessionID SessionPath NumSessions Modality http://neurobagel.org/vocab/81e47a09-658d-48ea-b7c5-8d54820d038e sub-01 34.1 http://purl.bioontology.org/ontology/SNOMEDCT/248152002 nan https://www.cognitiveatlas.org/task/id/4321, https://www.cognitiveatlas.org/task/id/1234 ses-01 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-01/ses-01 2 http://purl.org/nidash/nidm#FlowWeighted, http://purl.org/nidash/nidm#T1Weighted http://neurobagel.org/vocab/81e47a09-658d-48ea-b7c5-8d54820d038e sub-01 34.1 http://purl.bioontology.org/ontology/SNOMEDCT/248152002 nan https://www.cognitiveatlas.org/task/id/4321, https://www.cognitiveatlas.org/task/id/1234 ses-02 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-01/ses-02 2 http://purl.org/nidash/nidm#FlowWeighted, http://purl.org/nidash/nidm#T1Weighted http://neurobagel.org/vocab/81e47a09-658d-48ea-b7c5-8d54820d038e sub-02 nan http://purl.bioontology.org/ontology/SNOMEDCT/248153007 http://purl.bioontology.org/ontology/SNOMEDCT/49049000 https://www.cognitiveatlas.org/task/id/4321 ses-01 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-02/ses-01 2 http://purl.org/nidash/nidm#FlowWeighted, http://purl.org/nidash/nidm#T1Weighted http://neurobagel.org/vocab/81e47a09-658d-48ea-b7c5-8d54820d038e sub-02 nan http://purl.bioontology.org/ontology/SNOMEDCT/248153007 http://purl.bioontology.org/ontology/SNOMEDCT/49049000 https://www.cognitiveatlas.org/task/id/4321 ses-02 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-02/ses-02 2 http://purl.org/nidash/nidm#FlowWeighted, http://purl.org/nidash/nidm#T1Weighted http://neurobagel.org/vocab/81e47a09-658d-48ea-b7c5-8d54820d038e sub-03 22.1 nan nan https://www.cognitiveatlas.org/task/id/1234 ses-01 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-03/ses-01 2 http://purl.org/nidash/nidm#FlowWeighted, http://purl.org/nidash/nidm#T1Weighted http://neurobagel.org/vocab/81e47a09-658d-48ea-b7c5-8d54820d038e sub-03 22.1 nan nan https://www.cognitiveatlas.org/task/id/1234 ses-02 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-03/ses-02 2 http://purl.org/nidash/nidm#FlowWeighted, http://purl.org/nidash/nidm#T1Weighted http://neurobagel.org/vocab/81e47a09-658d-48ea-b7c5-8d54820d038e sub-04 21.1 http://purl.bioontology.org/ontology/SNOMEDCT/248152002 nan https://www.cognitiveatlas.org/task/id/4321 ses-01 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-04/ses-01 2 http://purl.org/nidash/nidm#FlowWeighted, http://purl.org/nidash/nidm#T1Weighted http://neurobagel.org/vocab/81e47a09-658d-48ea-b7c5-8d54820d038e sub-04 21.1 http://purl.bioontology.org/ontology/SNOMEDCT/248152002 nan https://www.cognitiveatlas.org/task/id/4321 ses-02 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-04/ses-02 2 http://purl.org/nidash/nidm#FlowWeighted, http://purl.org/nidash/nidm#T1Weighted http://neurobagel.org/vocab/81e47a09-658d-48ea-b7c5-8d54820d038e sub-05 42.5 http://purl.bioontology.org/ontology/SNOMEDCT/248153007 http://purl.bioontology.org/ontology/SNOMEDCT/49049000 https://www.cognitiveatlas.org/task/id/4321, https://www.cognitiveatlas.org/task/id/1234 ses-01 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-05/ses-01 2 http://purl.org/nidash/nidm#FlowWeighted, http://purl.org/nidash/nidm#T1Weighted http://neurobagel.org/vocab/81e47a09-658d-48ea-b7c5-8d54820d038e sub-05 42.5 http://purl.bioontology.org/ontology/SNOMEDCT/248153007 http://purl.bioontology.org/ontology/SNOMEDCT/49049000 https://www.cognitiveatlas.org/task/id/4321, https://www.cognitiveatlas.org/task/id/1234 ses-02 /data/neurobagel/bagel-cli/bids-examples/synthetic/sub-05/ses-02 2 http://purl.org/nidash/nidm#FlowWeighted, http://purl.org/nidash/nidm#T1Weighted"},{"location":"term_naming_standards/","title":"Neurobagel standards for controlled term naming","text":""},{"location":"term_naming_standards/#naming-conventions","title":"Naming conventions","text":""},{"location":"term_naming_standards/#namespace-prefixes","title":"Namespace prefixes","text":"<ul> <li>Names should be all lowercase (e.g., <code>nidm</code>, <code>cogatlas</code>)</li> </ul>"},{"location":"term_naming_standards/#properties-graph-edges","title":"Properties (graph \"edges\")","text":"<ul> <li>Names should adhere to camelCase (uses capitalized words except for the first word/letter)</li> <li>Should be a compound of:<ul> <li>a verb relevant to the property (e.g., hasAge, isSubjectGroup)</li> <li>the range of the property, (e.g.,hasDiagnosis points to a Diagnosis object)</li> </ul> </li> </ul> <p>What this might look like in semantic triples: <pre><code>&lt;Subject&gt; &lt;nb:hasDiagnosis&gt; &lt;snomed:1234&gt;\n&lt;snomed:1234&gt; &lt;rdf:type&gt; &lt;nb:Diagnosis&gt;\n</code></pre></p>"},{"location":"term_naming_standards/#classes-or-resources-graph-nodes","title":"Classes or resources (graph \"nodes\")","text":"<ul> <li>Names should adhere to PascalCase (each word capitalized)</li> <li>Where possible, simplify to a single word (e.g., <code>Diagnosis</code>, <code>Dataset</code>, <code>Sex</code>)</li> </ul> <p>Note</p> <p>Generally, we own the terms for properties and classes (e.g., Diagnosis, Assessment) but not the resources representing instances of classes such as specific diagnosis, sex, or assessment values (these are reused from existing vocabularies).</p> <p>In cases where we reuse a term for a class that comes from an existing controlled vocabulary, and that vocabulary follows a different naming convention (e.g., all lowercase), we should follow the existing naming convention.</p>"},{"location":"term_naming_standards/#currently-used-namespaces","title":"Currently used namespaces","text":"Prefix IRI Types of terms <code>nb</code> http://neurobagel.org/vocab/ Neurobagel-\"owned\" properties and classes <code>nbg</code> http://neurobagel.org/graph/ Neurobagel graph databases <code>snomed</code> http://purl.bioontology.org/ontology/SNOMEDCT/ diagnosis and sex values <code>ncit</code> http://ncicb.nci.nih.gov/xml/owl/EVS/Thesaurus.owl# group designation (e.g., healthy control) <code>nidm</code> http://purl.org/nidash/nidm# imaging modalities <code>cogatlas</code> https://www.cognitiveatlas.org/task/id/ cognitive assessments and tasks"},{"location":"term_naming_standards/#what-if-an-nb-term-already-exists-in-another-controlled-vocabulary","title":"What if an <code>nb</code> term already exists in another controlled vocabulary?","text":"<p>If there is an equivalent controlled term to one we are defining in a different namespace,  we document this and express their equivalence using <code>owl:sameAs</code>.</p> <p>Example: If our term is <code>nb:Subject</code> and <code>nidm:Subject</code> is conceptually equivalent: <pre><code>&lt;nb:12345&gt; a &lt;nb:Subject&gt;\n&lt;nb:Subject&gt; a &lt;rdfs:Resource&gt;;\n    &lt;owl:sameAs&gt; &lt;nidm:Subject&gt;\n</code></pre></p>"},{"location":"term_naming_standards/#other-general-guidelines","title":"Other general guidelines","text":"<ul> <li>Each property (edge) should use a single namespace for the resources it corresponds to</li> <li>Where possible, hardcode or refer to identifiers and not human-readable labels</li> </ul>"},{"location":"updating_dataset/","title":"Updating a harmonized dataset","text":"<p>When using Neurobagel tools on a dataset that is still undergoing data collection, you may need to update the Neurobagel annotations and/or graph-ready data for the dataset when you want to add new subjects or measurements or to correct mistakes in prior data versions.</p> <p>For any of the below types of changes, you will need to regenerate a graph-ready <code>.jsonld</code> file for the dataset which reflects the change.</p>"},{"location":"updating_dataset/#if-the-phenotypic-tabular-data-have-changed","title":"If the phenotypic (tabular) data have changed","text":"<p>If new variables have been added to the dataset such that there are new columns in the phenotypic TSV you previously annotated using Neurobagel's annotation tool, you will need to:  </p> <ol> <li> <p>Generate an updated data dictionary by annotating the new variables in your TSV following the annotation workflow</p> </li> <li> <p>Generate a new graph-ready data file for the dataset by re-running the CLI on your updated TSV and data dictionary</p> </li> </ol>"},{"location":"updating_dataset/#if-only-the-imaging-data-have-changed","title":"If only the imaging data have changed","text":"<p>If the BIDS data for a dataset have changed without changes in the corresponding phenotypic TSV (e.g., if new modalities or scans have been acquired for a subject), you have two options:</p> <ul> <li>If you still have access to the dataset's phenotypic JSONLD generated from the <code>pheno</code> command of the <code>bagel-cli</code> (step 1), you may choose to rerun only the <code>bids</code> CLI command on the updated BIDS directory.  This will generate a new graph-ready data file with updated imaging metadata of subjects.</li> </ul> <p>OR</p> <ul> <li>Rerun the CLI entirely (<code>pheno</code> and <code>bids</code> steps) to generate a new graph-ready data file for the dataset.</li> </ul> <p>When in doubt, rerun both CLI commands.</p>"},{"location":"updating_dataset/#if-only-the-subjects-have-changed","title":"If only the subjects have changed","text":"<p>If subjects have been added to or removed from the dataset but the phenotypic TSV is otherwise unchanged (i.e., only new or removed rows, without changes to the available variables), you will need to:</p> <ul> <li>Generate a new graph-ready data file for the dataset by re-running the CLI (<code>pheno</code> and <code>bids</code> steps) on your updated TSV and existing data dictionary</li> </ul>"},{"location":"updating_dataset/#updating-the-graph-database","title":"Updating the graph database","text":"<p>To allow easy (re-)uploading of the updated <code>.jsonld</code> for your dataset to a graph database, make a copy of it in a central directory on your research data fileserver for storing local Neurobagel <code>jsonld</code> datasets.  Then, follow the steps for uploading/updating a dataset in the graph database (needs to be completed by user with database write access).</p>"},{"location":"contributing/pull_requests/","title":"Pull requests","text":"<p>When submitting a pull request, titles should begin with a descriptive prefix (e.g., \"[ENH] Implement check for presence of a session ID column\").</p> <ul> <li><code>[ENH]:</code> Feature improvements or additions</li> <li><code>[REF]:</code> Refactoring existing code</li> <li><code>[TST]</code>: Updating or adding a test</li> <li><code>[CI]</code>: Automation-related changes</li> <li><code>[MNT]</code>: General maintenance not covered by <code>[REF]</code>, <code>[TST]</code>, or <code>[CI]</code></li> <li><code>[INF]</code>: Software or graph infrastructure-related changes</li> <li><code>[FIX]</code>: Bug fixes</li> <li><code>[MODEL]</code>: Updates or changes related to the Neurobagel data model</li> <li><code>[DOC]</code>: Documentation-only changes to a code repo (READMEs, within-code documentation, etc.)  Note: do not use for changes to the <code>neurobagel/documentation</code> repo. See below for PR prefixes for the documentation repo.</li> </ul>"},{"location":"contributing/pull_requests/#changes-to-documentation-pages","title":"Changes to documentation pages","text":"<p>In PRs for the Neurobagel documentation, using the <code>[DOC]</code> title prefix is discouraged as it is too broad.  Instead, for documentation content changes, the following prefixes can be used to specify the nature of the change:</p> <ul> <li><code>[ENH]</code>: Updating or adding new documentation</li> <li><code>[REF]</code>: Simplifying or restructuring documentation (i.e., pages, sections, paragraphs)</li> <li><code>[FIX]</code>: Fixing errors in the documentation</li> </ul>"},{"location":"contributing/pull_requests/#pull-request-reviews","title":"Pull request reviews","text":"<p>A developer will review each PR and may provide comments or suggestions for the PR author to address.</p> <p>Neurobagel PR reviews may use the following emoji signifiers:</p> <p>: Ready to merge or approved without suggestions</p> <p>: Some optional/suggested changes that could be nice to have but are not required to merge</p> <p>If (required) changes are requested, the PR author should re-request a review from the reviewer once the comments have been addressed.</p>"},{"location":"contributing/team/","title":"Our team","text":"<p>Neurobagel is a project originating from the ORIGAMI Lab at the Montreal Neurological Institute in collaboration with the Douglas Research Centre.</p>"},{"location":"contributing/team/#developers","title":"Developers","text":"<p>\ud83e\uddd1\u200d\ud83c\udf73 Core maintainer \ud83e\uddd1\u200d\ud83d\udd2c Principal Investigator</p> <sub>Sebastian Urchs</sub>\ud83e\uddd1\u200d\ud83c\udf73 <sub>Jonathan Armoza</sub>\ud83e\uddd1\u200d\ud83c\udf73 <sub>Arman Jahanpour</sub>\ud83e\uddd1\u200d\ud83c\udf73 <sub>Alyssa Dai</sub>\ud83e\uddd1\u200d\ud83c\udf73 <sub>Nikhil Bhagwat</sub> <sub>Michelle Wang</sub> <sub>Brent McPherson</sub> <sub>Remi Gau</sub> <sub>Jean-Baptise Poline</sub>\ud83e\uddd1\u200d\ud83d\udd2c"},{"location":"nipoppy/code_org/","title":"Code organization","text":""},{"location":"nipoppy/code_org/#code-organization","title":"Code organization","text":"<p>The Nipoppy codebase is divided into data processing <code>workflows</code> and data availability <code>trackers</code>.</p> <p><code>workflow</code></p> <ul> <li>MRI data organization (<code>dicom_org</code> and <code>bids_conv</code>)<ul> <li>Custom script to organize raw DICOMs (i.e. scanner output) into a flat participant-level directory. </li> <li>Convert DICOMs into BIDS using Heudiconv</li> </ul> </li> <li>MRI data processing (<code>proc_pipe</code>)<ul> <li>Runs a set of containerized MRI image processing pipelines </li> </ul> </li> <li>Tabular data (<code>tabular</code>)<ul> <li>Custom scripts to organize raw tabular data (e.g. clinial assessments)</li> <li>Custom scripts to normalize and standardize data and metadata for downstream harmonization (see NeuroBagel)</li> </ul> </li> </ul> <p><code>trackers</code></p> <ul> <li>Track available raw, standardized, and processed data</li> <li>Generate <code>bagels</code> for Neurobagel graph and dashboard. </li> </ul> <p>Legend - Red: dataset-specific code and configuration files - Yellow: Neurobagel interface</p> <p></p>"},{"location":"nipoppy/configs/","title":"Configs","text":""},{"location":"nipoppy/configs/#global-files","title":"Global files","text":"<p>Nipoppy requires two global files for specifying local data/container paths and recruitment manifest.</p>"},{"location":"nipoppy/configs/#global-configs-global_configsjson","title":"Global configs: <code>global_configs.json</code>","text":"<ul> <li>This is a dataset-specific file and needs to be modified based on local configs and paths</li> <li>This file is used as an input to all workflow runscripts to read, process and track available data</li> <li>Copy, rename, and populate sample_global_configs.json </li> <li>This file contains:<ul> <li>Name of the Nipoppy dataset (<code>DATASET_NAME</code>, e.g., <code>PPMI</code>)</li> <li>Path to the Nipoppy dataset (<code>DATASET_ROOT</code>)</li> <li>Path to a local <code>CONTAINER_STORE</code> comprising containers used by several workflow scripts</li> <li>Path to a Singularity executable (<code>SINGULARITY_PATH</code>)</li> <li>Path to a TemplateFlow directory, if using fMRIPrep (<code>TEMPLATEFLOW_DIR</code>)</li> <li>List of session IDs (<code>SESSION</code>, for MRI data) and visit IDs (<code>VISITS</code>, for tabular data)</li> <li>Containers + versions for BIDS conversion: HeuDiConv, BIDS validator (<code>BIDS</code>)</li> <li>List of processing pipelines + versions (<code>PROC_PIPELINES</code>)</li> <li>Information about tabular data (<code>TABULAR</code>)<ul> <li>Version and path to the data dictionary (<code>data_dictionary</code>)</li> </ul> </li> </ul> </li> </ul> <p>Suggestion</p> <p>Although not mandatory, for consistency the preferred location would be: <code>&lt;DATASET_ROOT&gt;/proc/global_configs.json</code>.</p>"},{"location":"nipoppy/configs/#sample-global_configsjson","title":"Sample <code>global_configs.json</code>","text":"<pre><code>{\n    \"DATASET_NAME\": \"MyDataset\",\n    \"DATASET_ROOT\": \"/path/to/MyDataset\",\n    \"CONTAINER_STORE\": \"/path/to/container_store\",\n    \"SINGULARITY_PATH\": \"singularity\",\n    \"TEMPLATEFLOW_DIR\": \"/path/to/templateflow\",\n\n    \"SESSIONS\": [\"1\",\"5\",\"7\",\"9\",\"11\"],\n\n    \"BIDS\": {\n        \"heudiconv\": {\n            \"VERSION\": \"0.11.6\",    \n            \"CONTAINER\": \"heudiconv_{}.sif\",\n            \"URL\": \"\"\n        },\n        \"validator\":{\n            \"CONTAINER\": \"bids_validator.sif\",\n            \"URL\": \"\"\n\n        }\n    },\n\n    \"PROC_PIPELINES\": {\n        \"mriqc\": {\n            \"VERSION\": \"\",\n            \"CONTAINER\": \"mriqc_{}.sif\",\n            \"URL\": \"\"\n        },\n        \"fmriprep\": {\n            \"VERSION\": \"20.2.7\",\n            \"CONTAINER\": \"fmriprep_{}.sif\",\n            \"URL\": \"\"\n        },\n        \"freesurfer\": {\n            \"VERSION\": \"6.0.1\",\n            \"CONTAINER\": \"fmriprep_{}.sif\",\n            \"URL\": \"\"\n        }\n    }\n}\n</code></pre>"},{"location":"nipoppy/configs/#participant-manifest-manifestcsv","title":"Participant manifest: <code>manifest.csv</code>","text":"<ul> <li>This list serves as the ground truth for subject and visit (i.e. session) availability</li> <li>Create the <code>manifest.csv</code> in <code>&lt;DATASET_ROOT&gt;/tabular/</code> comprising following columns<ul> <li><code>participant_id</code>: ID assigned during recruitment (at times used interchangeably with <code>subject_id</code>)</li> <li><code>visit</code>: label to denote participant visit for data acquisition (e.g. <code>\"baseline\"</code>, <code>\"m12\"</code>, <code>\"m24\"</code> or <code>\"V01\"</code>, <code>\"V02\"</code> etc.)</li> <li><code>session</code>: alternative naming for visit - typically used for imaging data to comply with BIDS standard</li> <li><code>datatype</code>: a list of acquired imaging datatype as defined by BIDS standard</li> <li><code>bids_id</code>: this is the <code>participant_id</code> after removing any non-alphanumeric character (e.g. \"-\" or \"_\") and attaching the <code>sub-</code> prefix. <code>participant_id</code> and <code>bids_id</code> in the manifest are used to link tabular and MRI data</li> </ul> </li> <li>New participant are appended upon recruitment as new rows</li> <li>Participants with multiple visits (i.e. sessions) should be added as separate rows</li> </ul>"},{"location":"nipoppy/configs/#sample-manifestcsv","title":"Sample <code>manifest.csv</code>","text":"participant_id visit session datatype bids_id 001 V01 ses-01 [\"anat\",\"dwi\",\"fmap\",\"func\"] sub-001 001 V02 ses-02 [\"anat\"] sub-001 002 V01 ses-01 [\"anat\",\"dwi\"] sub-002 002 V03 ses-03 [\"anat\",\"dwi\"] sub-002"},{"location":"nipoppy/data_org/","title":"Data organization","text":""},{"location":"nipoppy/data_org/#data-organization","title":"Data organization","text":"<p>An Nipoppy dataset consists of a specific directory structure to organize MRI and tabular data.</p> <p>Directories: </p> <ul> <li><code>tabular</code><ul> <li>contains <code>manifest.csv</code></li> <li><code>demographics</code>: contains demographic data (e.g. age, sex)</li> <li><code>assessments</code>: contains clinical assessments (e.g. MoCA) </li> </ul> </li> <li><code>downloads</code>: data dumps from remote data-stores (e.g. LONI)</li> <li><code>scratch</code>: space for un-organized data and wrangling</li> <li><code>dicom</code>: participant-level dicom dirs</li> <li><code>bids</code>: BIDS formatted dataset</li> <li><code>derivatives</code>: output of processing pipelines (e.g. fmriprep, mriqc)</li> <li><code>proc</code>: space for config and log files of the processing pipelines</li> <li><code>backups</code>: data backup space (tars)</li> <li><code>releases</code>: data releases (symlinks)</li> </ul> <p></p>"},{"location":"nipoppy/installation/","title":"Installation","text":""},{"location":"nipoppy/installation/#code-installation-and-dataset-setup","title":"Code installation and dataset setup","text":"<p>The Nipoppy workflow comprises a Nipoppy codebase that operates on a Nipoppy dataset with a specific directory structure (initialized with a <code>tree.py</code> script). </p>"},{"location":"nipoppy/installation/#nipoppy-codeenv-installation","title":"Nipoppy code+env installation","text":"<ol> <li>Change directory to where you want to clone this repo, e.g.: <code>cd /home/&lt;user&gt;/projects/&lt;my_project&gt;/code/</code></li> <li>Create a new venv: <code>python3 -m venv nipoppy_env</code></li> <li>Alternatively (if using Anaconda/Miniconda), create a <code>conda</code> environment: <code>conda create --name nipoppy_env python=3.9</code></li> <li>Activate your env: <code>source nipoppy_env/bin/activate</code> </li> <li>If using Anaconda/Miniconda: <code>conda activate nipoppy_env</code></li> <li>Clone this repo: <code>git clone https://github.com/neurodatascience/nipoppy.git</code></li> <li>Change directory to <code>nipoppy</code> </li> <li>Install python dependencies: <code>pip install -e .</code> </li> </ol>"},{"location":"nipoppy/installation/#nipoppy-dataset-directory-setup","title":"Nipoppy dataset directory setup","text":"<p>Run <code>tree.py</code> to create the Nipoppy dataset directory tree: <pre><code>python tree.py --nipoppy_root &lt;DATASET_ROOT&gt;\n</code></pre> Where - <code>DATASET_ROOT</code>: root (starting point) of the Nipoppy structured dataset</p> <p>Suggestion</p> <p>We suggest naming DATASET_ROOT directory after a study or a cohort. </p>"},{"location":"nipoppy/overview/","title":"Overview","text":""},{"location":"nipoppy/overview/#what-is-nipoppy-formerly-mr_proc","title":"What is Nipoppy (formerly mr_proc)?","text":"<p>Process long and prosper</p> <p>Nipoppy is a lightweight framework for analyzing (neuro)imaging and clinical data. It is designed to help users do the following:</p> <ol> <li>Curate and organize data into a standard directory structure</li> <li>Run data processing pipelines in a semi-automated and reproducible way</li> <li>Track availability (including processing status, if applicable) of raw, tabular and derived data</li> <li>Extract imaging features from MRI derivatives data for downstream analysis</li> </ol>"},{"location":"nipoppy/overview/#example-workflow","title":"Example workflow","text":"<p>Given a dataset to process, a typical workflow with Nipoppy can look like this:</p> <ol> <li>Fork the Nipoppy template code repository, then clone it<ul> <li>This repository contains scripts to run processing pipelines (and track their results) on BIDS data</li> </ul> </li> <li>Standardize raw imaging data: convert raw DICOM files into the NIfTI format and organize the dataset according to the BIDS standard<ul> <li>This requires some custom scripts, including the <code>heuristic.py</code> file for HeuDiConv</li> </ul> </li> <li>Run commonly-used image processing pipelines<ul> <li>Nipoppy currently supports FreeSurfer, fMRIPrep, TractoFlow, and MRIQC out-of-the-box, but new pipelines can be added by the user if needed</li> </ul> </li> <li>Organize demographic and clinical assessment data<ul> <li>This will most likely involve some custom data wrangling, for example to combine clinical assessment scores into a single file</li> </ul> </li> <li>Run tracker scripts to determine the availability of imaging (raw and/or processed) and/or tabular data<ul> <li>We call these availability/status metadata files \"bagels\" because they can be ingested by Neurobagel for dashboarding and querying participants across multiple studies</li> </ul> </li> </ol>"},{"location":"nipoppy/overview/#who-is-nipoppy-for","title":"Who is Nipoppy for?","text":"<p>Anyone who wants to process datasets with imaging data and/or use datasets processed with Nipoppy.</p> <p>Data managers</p> <ul> <li>People who process datasets, either for specific analyses or to share with others</li> <li>Example use cases:<ul> <li>Data curation<ul> <li>Download/organize raw DICOM files</li> <li>Convert raw data to a BIDS directory structure</li> <li>Organize clinical data</li> </ul> </li> <li>Data processing<ul> <li>Run pre-existing processing pipelines</li> <li>Add scripts to run custom pipelines</li> <li>Do cross-dataset processing: running the same pipelines/versions on different datasets so that the outputs can be used together</li> </ul> </li> <li>Data tracking<ul> <li>Check processing failures and relaunch processing</li> <li>Generate bagel files for Neurobagel</li> </ul> </li> </ul> </li> </ul> <p>Data users</p> <ul> <li>People who use tabular, derivative or other files produced by Nipoppy (e.g., from a shared Nipoppy-compliant dataset)</li> <li>Example use cases:<ul> <li>Data querying<ul> <li>Check availability of raw and/or derived data</li> <li>Check which pipelines and version have been run</li> </ul> </li> <li>Data analysis<ul> <li>Extract imaging features for downstream analyses from specific pipelines/versions</li> </ul> </li> </ul> </li> </ul>"},{"location":"nipoppy/overview/#how-does-nipoppy-work","title":"How does Nipoppy work?","text":""},{"location":"nipoppy/overview/#modules","title":"Modules","text":"<ol> <li><code>Code</code>: Codebase repo for running and tracking workflows<ul> <li>The codebase should start from the Nipoppy template repository. Additional custom scripts need to be added to process specific datasets.</li> </ul> </li> <li><code>Data</code>: Dataset organized in a specific directory structure<ul> <li>This contains the data only and should not contain any code</li> </ul> </li> <li><code>Containers</code>: Singularity/Apptainer containers encapsulating processing pipelines</li> </ol>"},{"location":"nipoppy/overview/#organization","title":"Organization","text":"<p>Organization of <code>Code</code>, <code>Data</code>, and <code>Container</code> modules</p> <p></p>"},{"location":"nipoppy/overview/#steps","title":"Steps","text":"<p>The Nipoppy workflow steps and linked identifiers (i.e. <code>participant_id</code>, <code>dicom_id</code>, <code>bids_id</code>) are shown below:</p> <p></p>"},{"location":"nipoppy/overview/#faq","title":"FAQ","text":"<ol> <li>Do I need to install anything to use Nipoppy?<ul> <li>Nipoppy requires Python 3 and Apptainer/Singularity to work.</li> </ul> </li> <li>Can Nipoppy process my entire dataset out-of-the-box?<ul> <li>No: every dataset is different, and it is virtually impossible to have a workflow flexible enough to work with any dataset format or structure. However, once the imaging data is converted to a standard BIDS structure, running the image processing pipelines should be very straightforward.</li> </ul> </li> <li>Do I need to follow all the steps listed in the example workflow?<ul> <li>No: the purpose of the example workflow is to illustrate what can be done with Nipoppy, but you can choose to only use it for specific features (e.g., tracking).</li> </ul> </li> <li>Can I run Nipoppy scripts in Windows/macOS?<ul> <li>Nipoppy is designed to run on the Linux operating system, and will likely not work on other operating systems. This is mainly because it relies on Singularity, which cannot run natively on Windows or macOS. It is probably possible to use Nipoppy with Windows/macOS (e.g., using virtual machines), but we do not recommend it.</li> </ul> </li> <li>Can I use Nipoppy on a cluster?<ul> <li>Yes, as long as the cluster has Apptainer/Singularity installed</li> </ul> </li> <li>Do I need to know how to use Apptainer/Singularity?<ul> <li>The Nipoppy code repo contains scripts that call Singularity to run image processing pipelines. Users are not required to use Singularity directly, though we encourage users to learn about containers and/or Singularity if they are not familiar with these terms.</li> </ul> </li> <li>I want to use Nipoppy with my own pipeline. Does it need to be containerized?<ul> <li>Although we recommend the use of containers to facilitate reproducibility, it is not a strict requirement. You can run your own pipelines any way you want (on the BIDS data or even raw data), though the outputs should be organized in the same way as the other pipelines (fMRIPrep, TractoFlow, etc.) if you want to use the tracking features.</li> </ul> </li> <li>What is Neurobagel and do I need to use it?<ul> <li>Neurobagel is a data harmonization project that includes tools to perform cross-datasets searches for imaging data availability. You do not need Neurobagel for Nipoppy, though some Nipoppy outputs (specifically the <code>bagel</code> tracking files) can be used as input to some Neurobagel tools.</li> </ul> </li> <li>How do I use the dashboard?<ul> <li>Simply visit https://dash.neurobagel.org (no installation required). More information about the dashboard can be found here.</li> </ul> </li> </ol>"},{"location":"nipoppy/workflow/bids_conv/","title":"BIDS conversion","text":""},{"location":"nipoppy/workflow/bids_conv/#objective","title":"Objective","text":"<p>Convert DICOMs to BIDS using Heudiconv (tutorial). </p>"},{"location":"nipoppy/workflow/bids_conv/#key-directories-and-files","title":"Key directories and files","text":"<ul> <li><code>&lt;DATASET_ROOT&gt;/dicom</code></li> <li><code>&lt;DATASET_ROOT&gt;/bids</code></li> <li><code>&lt;DATASET_ROOT&gt;/scratch/raw_dicoms/doughnut.csv</code></li> <li><code>heuristic.py</code></li> </ul>"},{"location":"nipoppy/workflow/bids_conv/#procedure","title":"Procedure","text":"<ol> <li>Ensure you have the appropriate HeuDiConv container listed in your <code>global_configs.json</code></li> <li>Use run_bids_conv.py to run HeuDiConv <code>stage_1</code> and <code>stage_2</code>.  </li> <li>Run <code>stage_1</code> to generate a list of available protocols from the DICOM header. These protocols are listed in <code>&lt;DATASET_ROOT&gt;/bids/.heudiconv/&lt;participant_id&gt;/info/dicominfo_ses-&lt;session_id&gt;.tsv</code></li> </ol> <p>Sample cmd: <pre><code>python run_bids_conv.py \\\n   --global_config &lt;global_config_file&gt; \\\n   --session_id &lt;session_id&gt; \\\n   --stage 1\n</code></pre></p> <p>Note</p> <p>If participants have multiple sessions (or visits), these need to be converted separately and combined post-hoc to avoid Heudiconv errors. </p> <ol> <li>Copy+Rename sample_heuristic.py to <code>heuristic.py</code> in the code repo itself. Then edit <code>./heuristic.py</code> to create a name-mapping (i.e. dictionary) for BIDS organization based on the list of available protocols. </li> </ol> <p>Note</p> <p>This file automatically gets copied into <code>&lt;DATASET_ROOT&gt;/proc/heuristic.py</code> to be seen by the Singularity container.</p> <ol> <li>Run <code>stage_2</code> to convert the dicoms into BIDS format based on the mapping from <code>heuristic.py</code>. This updates <code>&lt;DATASET_ROOT&gt;/scratch/raw_dicom/doughnut.csv</code> (sets <code>converted</code> column to <code>True</code>).</li> </ol> <p>Sample cmd: <pre><code>python run_bids_conv.py \\\n   --global_config &lt;global_config_file&gt; \\\n   --session_id &lt;session_id&gt; \\\n   --stage 2\n</code></pre></p> <p>Note</p> <p>Once <code>heuristic.py</code> is finalized, only <code>stage_2</code> needs to be run peridodically unless new scan protocol is added.</p>"},{"location":"nipoppy/workflow/dicom_org/","title":"DICOM organization","text":""},{"location":"nipoppy/workflow/dicom_org/#objective","title":"Objective","text":"<p>This is a dataset specific process and needs to be customized based on local scanner DICOM dumps and file naming. This organization should produce, for a given session, participant specific dicom dirs. Each of these participant-dir contains a flat list of dicoms for the participant for all available imaging modalities and scan protocols. The manifest is used to determine which new subject-session pairs need to be processed, and a <code>doughnut.csv</code> file is used to track the status for the DICOM reorganization and BIDS conversion steps.</p>"},{"location":"nipoppy/workflow/dicom_org/#key-directories-and-files","title":"Key directories and files","text":"<ul> <li><code>&lt;DATASET_ROOT&gt;/tabular/manifest.csv</code></li> <li><code>&lt;DATASET_ROOT&gt;/downloads</code></li> <li><code>&lt;DATASET_ROOT&gt;/scratch/raw_dicoms</code></li> <li><code>&lt;DATASET_ROOT&gt;/scratch/raw_dicoms/doughnut.csv</code></li> <li><code>&lt;DATASET_ROOT&gt;/dicom</code></li> </ul>"},{"location":"nipoppy/workflow/dicom_org/#procedure","title":"Procedure","text":"<ol> <li>Run <code>workflow/dicom_org/check_dicom_status.py</code> to update <code>doughnut.csv</code> based on the manifest. It will add new rows for any subject-session pair not already in the file.<ul> <li>To create the <code>doughnut.csv</code> for the first time, use the <code>--empty</code> argument. If processing has been done without updating <code>doughnut.csv</code>, use <code>--regenerate</code> to update it based on new files in the dataset.</li> </ul> </li> </ol> <p>Note</p> <p>The <code>doughnut.csv</code> file is used to track the multi-step conversion of raw DICOMs to BIDS: whether raw DICOMs have been downloaded to disk, re-organized into a directory structure accepted by HeuDiConv, and converted to BIDS. This file is updated automatically by scripts in <code>workflow/dicom_org</code> and <code>workflow/bids_conv</code>. Backups are created in case it is needed to revert to a previous version: they can be found in <code>&lt;DATASET_ROOT&gt;/scratch/raw_dicoms/.doughnuts</code>.</p> <p>Here is a sample <code>doughnut.csv</code> file:</p> participant_id session participant_dicom_dir dicom_id bids_id downloaded organized converted 001 ses-01 MyStudy_001_2021 sub-001 sub-001 True True True 001 ses-02 MyStudy_001_2022 sub-001 sub-001 True False False 002 ses-01 MyStudy_002_2021 sub-002 sub-002 True True False 002 ses-03 MyStudy_002_2024 sub-002 sub-002 False False False <ol> <li>Download DICOM dumps (e.g. ZIPs / tarballs) in the <code>&lt;DATASET_ROOT&gt;/downloads</code> directory. Different visits (i.e. sessions) must be downloaded in separate sub-directories and ideally named as listed in the <code>global_config.json</code>. The DICOM download and extraction process is highly dataset-dependent, and we recommend using custom scripts to automate it as much as possible.</li> <li>Extract (and rename if needed) all participants into <code>&lt;DATASET_ROOT&gt;/scratch/raw_dicoms</code> separately for each visit (i.e. session). <ul> <li>At this point, the <code>doughnut.csv</code> should have been updated to reflect the new downloads (<code>downloaded</code> column set to <code>True</code> where appropriate). We recommend doing this in the download script (i.e. in Step 2), but <code>workflow/dicom_org/check_dicom_status.py</code> can also be run with the <code>--regenerate</code> flag to search for the expected files (this can be very slow!).</li> </ul> </li> </ol> <p>Note</p> <p>IMPORTANT: the participant-level directory names should match <code>participant_id</code>s in the <code>manifest.csv</code>. It is recommended to use <code>participant_id</code> naming format to exclude any non-alphanumeric chacaters (e.g. \"-\" or \"_\"). If your participant_id does contain these characters, it is still recommended to remove them from the participant-level DICOM directory names (e.g., QPN_001 --&gt; QPN001).</p> <p>Note</p> <p>It is okay for the participant directory to have messy internal subdir tree with DICOMs from multiple modalities. (See data org schematic for details). The run script will search and validate all available DICOM files automatically. </p> <ol> <li>Run <code>run_dicom_org.py</code> to:<ul> <li>Search: Find all the DICOMs inside the participant directory. </li> <li>Validate: Excludes certain individual dicom files that are invalid or contain scanner-derived data not compatible with BIDS conversion.</li> <li>Symlink (default) or copy: Creates symlinks from <code>raw_dicoms/</code> to the <code>&lt;DATASET_ROOT&gt;/dicom</code>, where all participant specific dicoms are in a flat list. The symlinks are relative so that they are preserved in containers.</li> <li>Update status: if successful, set the <code>organized</code> column to <code>True</code> in <code>doughnut.csv</code>.</li> </ul> </li> </ol> <p>Sample cmd: <pre><code>python run_dicom_org.py \\\n    --global_config &lt;global_config_file&gt; \\\n    --session_id &lt;session_id&gt; \\\n</code></pre></p>"},{"location":"nipoppy/workflow/proc_pipe/fmriprep/","title":"fmriprep","text":""},{"location":"nipoppy/workflow/proc_pipe/fmriprep/#objective","title":"Objective","text":"<p>Run fMRIPrep pipeline on BIDS formatted dataset. Note that a standard fMRIPrep run also include FreeSurfer processing.</p>"},{"location":"nipoppy/workflow/proc_pipe/fmriprep/#key-directories-and-files","title":"Key directories and files","text":"<ul> <li><code>&lt;DATASET_ROOT&gt;/bids</code></li> <li><code>&lt;DATASET_ROOT&gt;/derivatives/fmriprep/</code></li> <li><code>&lt;DATASET_ROOT&gt;/derivatives/freesurfer/</code></li> <li><code>bids_filter.json</code></li> </ul>"},{"location":"nipoppy/workflow/proc_pipe/fmriprep/#procedure","title":"Procedure","text":"<ul> <li>Ensure you have the appropriate fMRIPrep container listed in your <code>global_configs.json</code> </li> <li>Use run_fmriprep.py script to run fmriprep pipeline. </li> <li>You can run \"anatomical only\" workflow by adding <code>--anat_only</code> flag</li> <li>(Optional) Copy+Rename sample_bids_filter.json to <code>bids_filter.json</code> in the code repo itself. Then edit <code>bids_filter.json</code> to filter certain modalities / acquisitions. This is common when you have multiple T1w acquisitions (e.g. Neuromelanin, SPIR etc.) for a given modality. </li> </ul> <p>Note</p> <p>When <code>--use_bids_filter</code> flag is set, this <code>bids_filter.json</code> is automatically copied into <code>&lt;DATASET_ROOT&gt;/bids/bids_filter.json</code> to be seen by the Singularity container.</p> <ul> <li>For FreeSurfer tasks, you need to have a license.txt file inside <code>&lt;DATASET_ROOT&gt;/derivatives/freesurfer/</code></li> <li>fMRIPrep manages brain-template spaces using TemplateFlow. These templates can be shared across studies and datasets. Use <code>global_configs.json</code> to specify path to <code>TEMPLATEFLOW_DIR</code> where these templates can reside. </li> </ul> <p>Note</p> <p>For machines with Internet connections, all required templates are automatically downloaded duing the fMRIPrep run.</p> <p>Sample cmd: <pre><code>python run_fmriprep.py \\\n   --global_config &lt;global_config_file&gt; \\\n   --participant_id MNI01 \\\n   --session_id 01 \\\n   --use_bids_filter \n</code></pre></p> <p>Note</p> <p>Unlike DICOM and BIDS run scripts, <code>run_fmriprep.py</code> can only process 1 participant at a time due to heavy compute requirements of fMRIPrep. For parallel processing on a cluster, sample HPC job scripts (slurm and sge) are provided in hpc subdir. </p> <p>Note</p> <p>You can change default run parameters in the run_fmriprep.py by looking at the documentation</p> <p>Note</p> <p>Clean up working dir (<code>fmriprep_wf</code>): fMRIPrep run generates huge number of intermediate files. You should remove those after successful run to free up space.</p>"},{"location":"nipoppy/workflow/proc_pipe/fmriprep/#fmriprep-tasks","title":"fMRIPrep tasks","text":"<ul> <li>Main MR processing tasks run by fmriprep (see fMRIPrep for details):<ul> <li>Preprocessing<ul> <li>Bias correction / Intensity normalization (N4)</li> <li>Brain extraction (ANTs)</li> <li>Spatial normalization to standard space(s)</li> </ul> </li> <li>Anatomical<ul> <li>Tissue segmentation (FAST)</li> <li>FreeSurfer recon-all</li> </ul> </li> <li>Functional<ul> <li>BOLD reference image estimation</li> <li>Head-motion estimation</li> <li>Slice time correction</li> <li>Susceptibility Distortion Correction (SDC)</li> <li>Pre-processed BOLD in native space</li> <li>EPI to T1w registration</li> <li>Resampling BOLD runs onto standard spaces</li> <li>EPI sampled to FreeSurfer surfaces</li> <li>Confounds estimation</li> <li>ICA-AROMA (not run by default)</li> </ul> </li> <li>Qualtiy Control<ul> <li>Visual reports</li> </ul> </li> </ul> </li> </ul>"},{"location":"nipoppy/workflow/proc_pipe/mriqc/","title":"mriqc","text":""},{"location":"nipoppy/workflow/proc_pipe/mriqc/#mriqc-image-processing-pipeline","title":"MRIQC image processing pipeline","text":"<p>MRIQC processes the participants and produces image quality metrics from T1w, T2w and BOLD data.</p>"},{"location":"nipoppy/workflow/proc_pipe/mriqc/#mriqc","title":"MRIQC","text":"<ul> <li>Use run_mriqc.py to run MRIQC pipeline directly or wrap the script in an SGE/Slurm script to run on cluster</li> </ul> <pre><code>python run_mriqc.py --global_config CONFIG.JSON --subject_id 001 --output_dir OUTPUT_DIR_PATH\n</code></pre> <ul> <li>Mandatory: Pass in the absolute path to the configuration containing the MRIQC container and data directory to <code>global_config</code></li> <li>Mandatory: Pass in the subject id to <code>participant_id</code></li> <li>Mandatory: Pass in the subject id to <code>session_id</code></li> <li>Mandatory: Pass in the absolute path to the output directory to <code>output_dir</code></li> </ul> <p>Note</p> <p>An example config is located here</p> <p>Sample cmd: <pre><code>python run_mriqc.py \\\n    --global_config GLOBAL_CONFIG \\\n    --participant_id SUBJECT_ID \\\n    --output_dir OUTPUT_DIR \\\n    --session_id SESSION_ID\n</code></pre></p> <p>Note</p> <p>A run for a participant is considered successful when the participant's log file reads <code>Participant level finished successfully</code></p>"},{"location":"nipoppy/workflow/proc_pipe/mriqc/#evaluate-mriqc-results","title":"Evaluate MRIQC Results","text":"<ul> <li>Use mriqc_tracker.py to determine how many subjects successfully passed through the MRIQC pipeline<ul> <li>Mandatory: Pass in the subject directory as an argument</li> </ul> </li> <li>After a successful run of the script, a dictionary called tracker_configs is returned contained whether the subject passed through the pipeline successfully</li> </ul> <p>Note</p> <p>Multiple sessions can be evaluated, but each session will require a new job running this script</p> <p>Sample cmd: <pre><code>&gt;&gt;&gt; results = {\"pipeline_complete': mriqc_tracker.eval_mriqc(subject_dir, session_id)}\n&gt;&gt;&gt; results\n SUCCESS\n&gt;&gt;&gt; results = {\"MRIQC_BOLD': mriqc_tracker.check_bold(subject_dir, session_id)}\n&gt;&gt;&gt; results\n FAIL\n</code></pre></p>"}]}